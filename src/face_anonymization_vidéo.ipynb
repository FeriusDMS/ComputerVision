{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "652bc25c",
   "metadata": {},
   "source": [
    "# üé¨ Face Detection and Anonymization System for Videos\n",
    "\n",
    "This notebook implements a computer vision system for detecting and anonymizing faces in **VIDEO files** using face swapping techniques.\n",
    "\n",
    "## Table of Contents\n",
    "1. [Setup and Imports](#setup)\n",
    "2. [Face Detection](#detection)\n",
    "3. [Face Landmark Detection](#landmarks)\n",
    "4. [Face Swapping](#swapping)\n",
    "5. [Video Processing](#video)\n",
    "6. [Testing and Evaluation](#testing)\n",
    "\n",
    "## üÜï Optimis√© pour Vid√©os\n",
    "- ‚úÖ Traitement frame par frame\n",
    "- ‚úÖ Barre de progression\n",
    "- ‚úÖ Export MP4 avec audio\n",
    "- ‚úÖ Preview des r√©sultats"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4643317",
   "metadata": {},
   "source": [
    "## 1. Setup and Imports <a name=\"setup\"></a>\n",
    "\n",
    "First, let's install and import all necessary libraries for our face detection and anonymization system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1d1012e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-20T14:01:43.472648Z",
     "start_time": "2025-11-20T14:01:39.708082Z"
    }
   },
   "outputs": [],
   "source": [
    "# Install required packages (compatible with Python 3.13)\n",
    "!pip install opencv-python opencv-contrib-python\n",
    "!pip install numpy matplotlib\n",
    "!pip install Pillow\n",
    "!pip install scipy\n",
    "!pip install face-alignment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f074aa6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-20T14:01:44.791226Z",
     "start_time": "2025-11-20T14:01:43.483206Z"
    }
   },
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import os\n",
    "from typing import List, Tuple, Optional\n",
    "from scipy.spatial import Delaunay\n",
    "import face_alignment\n",
    "from tqdm.notebook import tqdm\n",
    "import time\n",
    "from IPython.display import Video, display\n",
    "\n",
    "# Set matplotlib to display images inline\n",
    "%matplotlib inline\n",
    "\n",
    "# Set up plot style\n",
    "plt.style.use('default')\n",
    "plt.rcParams['figure.figsize'] = (12, 8)\n",
    "\n",
    "print(\"‚úì All libraries imported successfully!\")\n",
    "print(f\"OpenCV version: {cv2.__version__}\")\n",
    "print(f\"NumPy version: {np.__version__}\")\n",
    "print(f\"face-alignment library imported successfully\")\n",
    "print(\"‚úì Video processing libraries ready!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "787e4f6e",
   "metadata": {},
   "source": [
    "### Helper Functions for Visualization\n",
    "\n",
    "Let's create some utility functions to display frames and results properly in the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9dd6147",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-20T14:01:47.225556Z",
     "start_time": "2025-11-20T14:01:47.221860Z"
    }
   },
   "outputs": [],
   "source": [
    "def display_image(image, title=\"Frame\", cmap=None, figsize=(10, 6)):\n",
    "    \"\"\"\n",
    "    Display a frame/image using matplotlib.\n",
    "    \n",
    "    Args:\n",
    "        image: Image array (BGR or RGB) - can be a video frame\n",
    "        title: Title for the plot\n",
    "        cmap: Color map (None for RGB, 'gray' for grayscale)\n",
    "        figsize: Figure size tuple\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=figsize)\n",
    "    if len(image.shape) == 3 and image.shape[2] == 3:\n",
    "        # Convert BGR to RGB for display\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "    plt.imshow(image, cmap=cmap)\n",
    "    plt.title(title, fontsize=14, fontweight='bold')\n",
    "    plt.axis('off')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def display_images_grid(images, titles, rows=1, cols=2, figsize=(15, 8)):\n",
    "    \"\"\"\n",
    "    Display multiple frames/images in a grid.\n",
    "    \n",
    "    Args:\n",
    "        images: List of images (can be video frames)\n",
    "        titles: List of titles\n",
    "        rows: Number of rows\n",
    "        cols: Number of columns\n",
    "        figsize: Figure size tuple\n",
    "    \"\"\"\n",
    "    fig, axes = plt.subplots(rows, cols, figsize=figsize)\n",
    "    axes = axes.flatten() if rows * cols > 1 else [axes]\n",
    "    \n",
    "    for idx, (image, title) in enumerate(zip(images, titles)):\n",
    "        if idx < len(axes):\n",
    "            if len(image.shape) == 3 and image.shape[2] == 3:\n",
    "                image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "            axes[idx].imshow(image)\n",
    "            axes[idx].set_title(title, fontsize=12, fontweight='bold')\n",
    "            axes[idx].axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "print(\"‚úì Visualization functions defined!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86e7d000",
   "metadata": {},
   "source": [
    "## 2. Face Detection <a name=\"detection\"></a>\n",
    "\n",
    "We'll implement face detection using multiple methods:\n",
    "- Haar Cascade (OpenCV)\n",
    "- DNN-based face detector (more accurate)\n",
    "\n",
    "### 2.1 Haar Cascade Face Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56e14f6e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-20T14:01:47.283457Z",
     "start_time": "2025-11-20T14:01:47.276528Z"
    }
   },
   "outputs": [],
   "source": [
    "class FaceDetector:\n",
    "    \"\"\"\n",
    "    A class for detecting faces in images using various methods.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, method='haar'):\n",
    "        \"\"\"\n",
    "        Initialize the face detector.\n",
    "        \n",
    "        Args:\n",
    "            method: Detection method ('haar' or 'dnn')\n",
    "        \"\"\"\n",
    "        self.method = method\n",
    "        \n",
    "        if method == 'haar':\n",
    "            # Load Haar Cascade classifier\n",
    "            self.face_cascade = cv2.CascadeClassifier(\n",
    "                cv2.data.haarcascades + 'haarcascade_frontalface_default.xml'\n",
    "            )\n",
    "            print(\"‚úì Haar Cascade loaded\")\n",
    "            \n",
    "        elif method == 'dnn':\n",
    "            # Load DNN model for face detection\n",
    "            self.model_file = \"res10_300x300_ssd_iter_140000.caffemodel\"\n",
    "            self.config_file = \"deploy.prototxt\"\n",
    "            \n",
    "            # Note: You'll need to download these files\n",
    "            # We'll provide download instructions\n",
    "            try:\n",
    "                self.net = cv2.dnn.readNetFromCaffe(self.config_file, self.model_file)\n",
    "                print(\"‚úì DNN model loaded\")\n",
    "            except:\n",
    "                print(\"‚ö† DNN model files not found. Using Haar Cascade instead.\")\n",
    "                self.method = 'haar'\n",
    "                self.face_cascade = cv2.CascadeClassifier(\n",
    "                    cv2.data.haarcascades + 'haarcascade_frontalface_default.xml'\n",
    "                )\n",
    "    \n",
    "    def detect_faces_haar(self, image, scale_factor=1.1, min_neighbors=5, min_size=(30, 30)):\n",
    "        \"\"\"\n",
    "        Detect faces using Haar Cascade.\n",
    "        \n",
    "        Args:\n",
    "            image: Input image (BGR)\n",
    "            scale_factor: Parameter specifying how much the image size is reduced at each image scale\n",
    "            min_neighbors: Parameter specifying how many neighbors each candidate rectangle should have\n",
    "            min_size: Minimum possible object size\n",
    "            \n",
    "        Returns:\n",
    "            List of face bounding boxes [(x, y, w, h), ...]\n",
    "        \"\"\"\n",
    "        # Convert to grayscale\n",
    "        gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "        \n",
    "        # Detect faces\n",
    "        faces = self.face_cascade.detectMultiScale(\n",
    "            gray,\n",
    "            scaleFactor=scale_factor,\n",
    "            minNeighbors=min_neighbors,\n",
    "            minSize=min_size\n",
    "        )\n",
    "        \n",
    "        return faces\n",
    "    \n",
    "    def detect_faces_dnn(self, image, confidence_threshold=0.5):\n",
    "        \"\"\"\n",
    "        Detect faces using DNN model.\n",
    "        \n",
    "        Args:\n",
    "            image: Input image (BGR)\n",
    "            confidence_threshold: Minimum confidence for detection\n",
    "            \n",
    "        Returns:\n",
    "            List of face bounding boxes [(x, y, w, h), ...]\n",
    "        \"\"\"\n",
    "        h, w = image.shape[:2]\n",
    "        \n",
    "        # Prepare blob for DNN\n",
    "        blob = cv2.dnn.blobFromImage(\n",
    "            cv2.resize(image, (300, 300)),\n",
    "            1.0,\n",
    "            (300, 300),\n",
    "            (104.0, 177.0, 123.0)\n",
    "        )\n",
    "        \n",
    "        # Forward pass\n",
    "        self.net.setInput(blob)\n",
    "        detections = self.net.forward()\n",
    "        \n",
    "        # Extract faces with high confidence\n",
    "        faces = []\n",
    "        for i in range(detections.shape[2]):\n",
    "            confidence = detections[0, 0, i, 2]\n",
    "            \n",
    "            if confidence > confidence_threshold:\n",
    "                box = detections[0, 0, i, 3:7] * np.array([w, h, w, h])\n",
    "                (x, y, x2, y2) = box.astype(\"int\")\n",
    "                faces.append((x, y, x2-x, y2-y))\n",
    "        \n",
    "        return np.array(faces)\n",
    "    \n",
    "    def detect(self, image, **kwargs):\n",
    "        \"\"\"\n",
    "        Detect faces using the selected method.\n",
    "        \n",
    "        Args:\n",
    "            image: Input image (BGR)\n",
    "            **kwargs: Additional parameters for the detection method\n",
    "            \n",
    "        Returns:\n",
    "            List of face bounding boxes\n",
    "        \"\"\"\n",
    "        if self.method == 'haar':\n",
    "            return self.detect_faces_haar(image, **kwargs)\n",
    "        elif self.method == 'dnn':\n",
    "            return self.detect_faces_dnn(image, **kwargs)\n",
    "    \n",
    "    def draw_faces(self, image, faces, color=(0, 255, 0), thickness=2):\n",
    "        \"\"\"\n",
    "        Draw bounding boxes around detected faces.\n",
    "        \n",
    "        Args:\n",
    "            image: Input image\n",
    "            faces: List of face bounding boxes\n",
    "            color: Rectangle color (BGR)\n",
    "            thickness: Rectangle thickness\n",
    "            \n",
    "        Returns:\n",
    "            Image with drawn rectangles\n",
    "        \"\"\"\n",
    "        result = image.copy()\n",
    "        \n",
    "        for (x, y, w, h) in faces:\n",
    "            cv2.rectangle(result, (x, y), (x+w, y+h), color, thickness)\n",
    "            # Add label\n",
    "            cv2.putText(result, 'Face', (x, y-10), \n",
    "                       cv2.FONT_HERSHEY_SIMPLEX, 0.5, color, 2)\n",
    "        \n",
    "        return result\n",
    "\n",
    "print(\"‚úì FaceDetector class defined!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28fb855a",
   "metadata": {},
   "source": [
    "### 2.2 Test Face Detection\n",
    "\n",
    "Let's test the face detector with a sample frame from a video."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca31d2d5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-20T14:01:47.347931Z",
     "start_time": "2025-11-20T14:01:47.336493Z"
    }
   },
   "outputs": [],
   "source": [
    "# Initialize the face detector\n",
    "detector = FaceDetector(method='haar')\n",
    "\n",
    "# For testing, you can load a video and extract a frame\n",
    "# Example: cap = cv2.VideoCapture('video.mp4')\n",
    "#          ret, frame = cap.read()\n",
    "print(\"Face detector initialized!\")\n",
    "print(\"To test, load a video frame using: cap = cv2.VideoCapture('path/to/video.mp4')\")\n",
    "print(\"                                    ret, frame = cap.read()\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0169d22",
   "metadata": {},
   "source": [
    "## 3. Face Landmark Detection <a name=\"landmarks\"></a>\n",
    "\n",
    "Face landmarks are key points on the face (eyes, nose, mouth, etc.) that help us align and swap faces accurately.\n",
    "\n",
    "We'll use the face-alignment library which provides state-of-the-art facial landmark detection.\n",
    "\n",
    "### üÜï NOUVEAU: D√©tection avec Front Inclus\n",
    "\n",
    "**Innovation**: Contrairement aux syst√®mes classiques qui d√©tectent 68 points, notre syst√®me d√©tecte automatiquement **73 points incluant le front**!\n",
    "\n",
    "#### üìä Structure des Landmarks:\n",
    "- **Points 0-67**: Landmarks faciaux standard (68 points)\n",
    "  - Points 0-16: Contour du visage\n",
    "  - Points 17-26: Sourcils\n",
    "  - Points 27-35: Nez\n",
    "  - Points 36-47: Yeux\n",
    "  - Points 48-67: Bouche\n",
    "- **Points 68-72**: Zone du front (5 points nouveaux) ‚≠ê\n",
    "  - Extension automatique au-dessus des sourcils\n",
    "  - Couverture compl√®te de la zone frontale\n",
    "\n",
    "#### ‚ú® Avantages:\n",
    "‚úÖ Front inclus d√®s la d√©tection (pas besoin d'extension manuelle)  \n",
    "‚úÖ Visualisation claire (points du front en ROUGE)  \n",
    "‚úÖ Compatible avec le face swapping  \n",
    "‚úÖ Meilleure couverture pour l'anonymisation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e17ada46",
   "metadata": {},
   "source": [
    "### üßπ GPU Memory Management\n",
    "\n",
    "If you encounter CUDA out of memory errors, run this cell to clear GPU cache."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36a06a9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean GPU memory if needed\n",
    "import gc\n",
    "\n",
    "def clear_gpu_memory():\n",
    "    \"\"\"Clear GPU memory and Python garbage collection\"\"\"\n",
    "    try:\n",
    "        import torch\n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.empty_cache()\n",
    "            torch.cuda.ipc_collect()\n",
    "            print(\"‚úì GPU cache cleared\")\n",
    "            \n",
    "            # Show memory stats\n",
    "            for i in range(torch.cuda.device_count()):\n",
    "                total = torch.cuda.get_device_properties(i).total_memory / (1024**3)\n",
    "                reserved = torch.cuda.memory_reserved(i) / (1024**3)\n",
    "                allocated = torch.cuda.memory_allocated(i) / (1024**3)\n",
    "                free = total - allocated\n",
    "                \n",
    "                print(f\"\\nüìä GPU {i} Memory:\")\n",
    "                print(f\"  Total: {total:.2f} GiB\")\n",
    "                print(f\"  Allocated: {allocated:.2f} GiB\")\n",
    "                print(f\"  Reserved: {reserved:.2f} GiB\")\n",
    "                print(f\"  Free: {free:.2f} GiB\")\n",
    "        else:\n",
    "            print(\"No CUDA GPU available\")\n",
    "    except Exception as e:\n",
    "        print(f\"Could not clear GPU memory: {e}\")\n",
    "    \n",
    "    # Python garbage collection\n",
    "    gc.collect()\n",
    "    print(\"‚úì Python garbage collection done\")\n",
    "\n",
    "# Run cleanup\n",
    "clear_gpu_memory()\n",
    "\n",
    "print(\"\\nüí° TIP: If you still get CUDA errors, restart the kernel and use CPU mode\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6aa2de98",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-20T14:01:48.663161Z",
     "start_time": "2025-11-20T14:01:47.840939Z"
    }
   },
   "outputs": [],
   "source": [
    "# Initialize face-alignment with GPU optimization\n",
    "print(\"Initializing face-alignment...\")\n",
    "\n",
    "# Clean up GPU memory first\n",
    "try:\n",
    "    import torch\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "        print(\"  GPU cache cleared\")\n",
    "        \n",
    "        # Set memory efficient settings\n",
    "        torch.backends.cudnn.benchmark = False\n",
    "        torch.backends.cudnn.deterministic = True\n",
    "        \n",
    "        # Try to use GPU with face-alignment\n",
    "        try:\n",
    "            fa = face_alignment.FaceAlignment(\n",
    "                face_alignment.LandmarksType.TWO_D, \n",
    "                flip_input=False, \n",
    "                device='cuda',\n",
    "                face_detector='sfd'  # Use SFD detector which is more memory efficient\n",
    "            )\n",
    "            print(\"‚úì face-alignment initialized with CUDA/GPU!\")\n",
    "            print(f\"  GPU: {torch.cuda.get_device_name(0)}\")\n",
    "            print(f\"  Memory: {torch.cuda.get_device_properties(0).total_memory / (1024**3):.2f} GiB\")\n",
    "        except RuntimeError as e:\n",
    "            if \"out of memory\" in str(e).lower():\n",
    "                # Fallback to CPU if GPU memory is insufficient\n",
    "                print(\"  ‚ö† GPU out of memory, falling back to CPU\")\n",
    "                torch.cuda.empty_cache()\n",
    "                fa = face_alignment.FaceAlignment(\n",
    "                    face_alignment.LandmarksType.TWO_D, \n",
    "                    flip_input=False, \n",
    "                    device='cpu'\n",
    "                )\n",
    "                print(\"‚úì face-alignment initialized with CPU\")\n",
    "            else:\n",
    "                raise\n",
    "    else:\n",
    "        print(\"  No CUDA GPU available\")\n",
    "        fa = face_alignment.FaceAlignment(\n",
    "            face_alignment.LandmarksType.TWO_D, \n",
    "            flip_input=False, \n",
    "            device='cpu'\n",
    "        )\n",
    "        print(\"‚úì face-alignment initialized with CPU\")\n",
    "except Exception as e:\n",
    "    print(f\"  Error: {e}\")\n",
    "    # Final fallback to CPU\n",
    "    fa = face_alignment.FaceAlignment(\n",
    "        face_alignment.LandmarksType.TWO_D, \n",
    "        flip_input=False, \n",
    "        device='cpu'\n",
    "    )\n",
    "    print(\"‚úì face-alignment initialized with CPU (fallback)\")\n",
    "\n",
    "print(\"‚úì Ready to detect facial landmarks!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b0f28fa",
   "metadata": {},
   "source": [
    "### ‚ö° GPU Optimization Settings\n",
    "\n",
    "Configure PyTorch and CUDA for optimal video processing performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3eae95d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GPU optimization for video processing\n",
    "import torch\n",
    "import gc\n",
    "\n",
    "def optimize_gpu_memory():\n",
    "    \"\"\"Optimize GPU memory settings for video processing\"\"\"\n",
    "    if torch.cuda.is_available():\n",
    "        # Enable memory efficient settings\n",
    "        torch.backends.cudnn.benchmark = True  # Auto-tune for best performance\n",
    "        torch.backends.cudnn.enabled = True\n",
    "        \n",
    "        # Set memory allocation strategy\n",
    "        import os\n",
    "        os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'expandable_segments:True'\n",
    "        \n",
    "        # Clear cache\n",
    "        torch.cuda.empty_cache()\n",
    "        gc.collect()\n",
    "        \n",
    "        print(\"‚úì GPU optimizations applied:\")\n",
    "        print(\"  - CuDNN benchmark: enabled (faster processing)\")\n",
    "        print(\"  - Memory allocation: expandable segments\")\n",
    "        print(\"  - Cache cleared\")\n",
    "        \n",
    "        # Show GPU info\n",
    "        for i in range(torch.cuda.device_count()):\n",
    "            props = torch.cuda.get_device_properties(i)\n",
    "            total_mem = props.total_memory / (1024**3)\n",
    "            allocated = torch.cuda.memory_allocated(i) / (1024**3)\n",
    "            cached = torch.cuda.memory_reserved(i) / (1024**3)\n",
    "            free = total_mem - allocated\n",
    "            \n",
    "            print(f\"\\n  GPU {i}: {props.name}\")\n",
    "            print(f\"    Total: {total_mem:.2f} GiB\")\n",
    "            print(f\"    Free: {free:.2f} GiB\")\n",
    "            print(f\"    Allocated: {allocated:.2f} GiB\")\n",
    "            print(f\"    Cached: {cached:.2f} GiB\")\n",
    "    else:\n",
    "        print(\"No CUDA GPU available - using CPU\")\n",
    "        print(\"‚ö† Warning: CPU processing will be much slower for videos\")\n",
    "\n",
    "# Apply optimizations\n",
    "optimize_gpu_memory()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2a52e43",
   "metadata": {},
   "source": [
    "### ‚úÖ Optimisations GPU Appliqu√©es\n",
    "\n",
    "Le syst√®me est maintenant optimis√© pour le traitement vid√©o avec GPU :\n",
    "\n",
    "**üöÄ Optimisations activ√©es :**\n",
    "1. **CuDNN Benchmark** : Auto-tune pour les meilleures performances\n",
    "2. **Expandable Segments** : Gestion flexible de la m√©moire GPU\n",
    "3. **Cache GPU** : Nettoyage automatique apr√®s chaque d√©tection de landmarks\n",
    "4. **D√©tecteur SFD** : Plus efficace en m√©moire que les autres d√©tecteurs\n",
    "5. **Fallback CPU** : Bascule automatique vers CPU si GPU satur√©\n",
    "\n",
    "**üìä Performances attendues (avec GPU) :**\n",
    "- Vid√©o 1080p : ~15-30 FPS\n",
    "- Vid√©o 720p : ~30-60 FPS\n",
    "- Vid√©o 480p : ~60+ FPS\n",
    "\n",
    "**‚ö†Ô∏è Si vous avez encore des erreurs GPU :**\n",
    "- R√©duisez la r√©solution de la vid√©o\n",
    "- Fermez les autres programmes utilisant le GPU\n",
    "- Red√©marrez le kernel Jupyter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af01daf9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-20T14:01:48.728371Z",
     "start_time": "2025-11-20T14:01:48.719354Z"
    }
   },
   "outputs": [],
   "source": [
    "class FaceLandmarkDetector:\n",
    "    \"\"\"\n",
    "    A class for detecting facial landmarks using face-alignment library.\n",
    "    Les landmarks incluent automatiquement le front (73 points au total: 68 standard + 5 pour le front).\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, fa_detector):\n",
    "        \"\"\"\n",
    "        Initialize the landmark detector using face-alignment.\n",
    "        \n",
    "        Args:\n",
    "            fa_detector: face_alignment.FaceAlignment instance\n",
    "        \"\"\"\n",
    "        self.fa = fa_detector\n",
    "        print(\"‚úì FaceLandmarkDetector initialized with face-alignment!\")\n",
    "        print(\"  ‚Üí Les landmarks incluront automatiquement le front (73 points)\")\n",
    "    \n",
    "    def extend_landmarks_with_forehead(self, landmarks, image_shape):\n",
    "        \"\"\"\n",
    "        Extend facial landmarks to include forehead points.\n",
    "        \n",
    "        Args:\n",
    "            landmarks: Original facial landmark points (68 points)\n",
    "            image_shape: Shape of the image (height, width)\n",
    "            \n",
    "        Returns:\n",
    "            Extended landmarks including forehead points (73 points)\n",
    "        \"\"\"\n",
    "        landmarks = np.array(landmarks, dtype=np.float32)\n",
    "        \n",
    "        # Get face outline points (indices 0-16 for standard 68-point model)\n",
    "        if len(landmarks) >= 27:\n",
    "            # Calculate forehead extension based on face outline\n",
    "            left_face = landmarks[0]  # Left side of face\n",
    "            right_face = landmarks[16]  # Right side of face\n",
    "            \n",
    "            # Get eyebrow points for reference\n",
    "            left_eyebrow_top = landmarks[19]  # Left eyebrow center\n",
    "            right_eyebrow_top = landmarks[24]  # Right eyebrow center\n",
    "            nose_bridge = landmarks[27]  # Top of nose bridge\n",
    "            \n",
    "            # Calculate forehead height (extend upward from eyebrows)\n",
    "            eyebrow_avg_y = (left_eyebrow_top[1] + right_eyebrow_top[1]) / 2\n",
    "            forehead_height = abs(nose_bridge[1] - eyebrow_avg_y) * 1.5\n",
    "            \n",
    "            # Create forehead points\n",
    "            forehead_points = []\n",
    "            \n",
    "            # Left forehead point\n",
    "            forehead_points.append([\n",
    "                left_face[0] + (landmarks[1][0] - left_face[0]) * 0.3,\n",
    "                eyebrow_avg_y - forehead_height\n",
    "            ])\n",
    "            \n",
    "            # Left-center forehead point\n",
    "            forehead_points.append([\n",
    "                left_eyebrow_top[0],\n",
    "                eyebrow_avg_y - forehead_height * 1.1\n",
    "            ])\n",
    "            \n",
    "            # Center forehead point\n",
    "            forehead_points.append([\n",
    "                (left_eyebrow_top[0] + right_eyebrow_top[0]) / 2,\n",
    "                eyebrow_avg_y - forehead_height * 1.2\n",
    "            ])\n",
    "            \n",
    "            # Right-center forehead point\n",
    "            forehead_points.append([\n",
    "                right_eyebrow_top[0],\n",
    "                eyebrow_avg_y - forehead_height * 1.1\n",
    "            ])\n",
    "            \n",
    "            # Right forehead point\n",
    "            forehead_points.append([\n",
    "                right_face[0] - (right_face[0] - landmarks[15][0]) * 0.3,\n",
    "                eyebrow_avg_y - forehead_height\n",
    "            ])\n",
    "            \n",
    "            # Add forehead points to landmarks\n",
    "            extended_landmarks = np.vstack([landmarks, forehead_points])\n",
    "            \n",
    "            return extended_landmarks.astype(np.int32)\n",
    "        \n",
    "        # Fallback: return original landmarks if extension fails\n",
    "        return landmarks.astype(np.int32)\n",
    "    \n",
    "    def get_landmarks(self, image, face_rect=None, include_forehead=True):\n",
    "        \"\"\"\n",
    "        Detect facial landmarks in an image with GPU memory optimization.\n",
    "        \n",
    "        Args:\n",
    "            image: Input image (BGR format from OpenCV)\n",
    "            face_rect: Face bounding box (x, y, w, h) - not used with face-alignment but kept for compatibility\n",
    "            include_forehead: If True, automatically extend landmarks to include forehead (default: True)\n",
    "            \n",
    "        Returns:\n",
    "            Array of landmark points [(x, y), ...]\n",
    "            - 73 points if include_forehead=True (68 standard + 5 forehead)\n",
    "            - 68 points if include_forehead=False (standard face-alignment)\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # Convert BGR to RGB for face-alignment\n",
    "            image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "            \n",
    "            # Detect landmarks\n",
    "            landmarks = self.fa.get_landmarks(image_rgb)\n",
    "            \n",
    "            # Clean up GPU memory after detection to avoid OOM\n",
    "            try:\n",
    "                import torch\n",
    "                if torch.cuda.is_available():\n",
    "                    torch.cuda.empty_cache()\n",
    "            except:\n",
    "                pass\n",
    "            \n",
    "            if landmarks is not None and len(landmarks) > 0:\n",
    "                # Get the first face's landmarks\n",
    "                base_landmarks = landmarks[0].astype(np.int32)\n",
    "                \n",
    "                # Extend with forehead if requested\n",
    "                if include_forehead:\n",
    "                    extended = self.extend_landmarks_with_forehead(base_landmarks, image.shape)\n",
    "                    return extended\n",
    "                else:\n",
    "                    return base_landmarks\n",
    "            else:\n",
    "                return None\n",
    "                \n",
    "        except Exception as e:\n",
    "            # Clean up GPU memory on error\n",
    "            try:\n",
    "                import torch\n",
    "                if torch.cuda.is_available():\n",
    "                    torch.cuda.empty_cache()\n",
    "            except:\n",
    "                pass\n",
    "            \n",
    "            if \"out of memory\" in str(e).lower():\n",
    "                print(f\"‚ö† GPU out of memory - try reducing video resolution or using CPU\")\n",
    "            else:\n",
    "                print(f\"‚ö† Error detecting landmarks: {e}\")\n",
    "            return None\n",
    "    \n",
    "    def draw_landmarks(self, image, landmarks, color=(0, 255, 0), radius=2):\n",
    "        \"\"\"\n",
    "        Draw facial landmarks on an image.\n",
    "        \n",
    "        Args:\n",
    "            image: Input image\n",
    "            landmarks: Array of landmark points\n",
    "            color: Color for drawing (BGR)\n",
    "            radius: Radius of landmark points\n",
    "            \n",
    "        Returns:\n",
    "            Image with drawn landmarks\n",
    "        \"\"\"\n",
    "        result = image.copy()\n",
    "        \n",
    "        if landmarks is not None and len(landmarks) > 0:\n",
    "            # Draw landmark points\n",
    "            num_landmarks = len(landmarks)\n",
    "            \n",
    "            # Draw first 68 points in green\n",
    "            for i in range(min(68, num_landmarks)):\n",
    "                x, y = landmarks[i]\n",
    "                cv2.circle(result, (int(x), int(y)), radius, color, -1)\n",
    "            \n",
    "            # Draw forehead points in red if present (points 68-72)\n",
    "            if num_landmarks > 68:\n",
    "                for i in range(68, num_landmarks):\n",
    "                    x, y = landmarks[i]\n",
    "                    cv2.circle(result, (int(x), int(y)), radius + 1, (0, 0, 255), -1)\n",
    "            \n",
    "            # Draw connecting lines for major facial features (first 68 points)\n",
    "            if num_landmarks >= 68:  # Standard 68-point model\n",
    "                # Face outline\n",
    "                for i in range(16):\n",
    "                    cv2.line(result, tuple(landmarks[i]), tuple(landmarks[i+1]), color, 1)\n",
    "                \n",
    "                # Eyebrows\n",
    "                for i in range(17, 21):\n",
    "                    cv2.line(result, tuple(landmarks[i]), tuple(landmarks[i+1]), color, 1)\n",
    "                for i in range(22, 26):\n",
    "                    cv2.line(result, tuple(landmarks[i]), tuple(landmarks[i+1]), color, 1)\n",
    "                \n",
    "                # Nose\n",
    "                for i in range(27, 30):\n",
    "                    cv2.line(result, tuple(landmarks[i]), tuple(landmarks[i+1]), color, 1)\n",
    "                for i in range(31, 35):\n",
    "                    cv2.line(result, tuple(landmarks[i]), tuple(landmarks[i+1]), color, 1)\n",
    "                \n",
    "                # Eyes\n",
    "                for i in range(36, 41):\n",
    "                    cv2.line(result, tuple(landmarks[i]), tuple(landmarks[i+1]), color, 1)\n",
    "                cv2.line(result, tuple(landmarks[41]), tuple(landmarks[36]), color, 1)\n",
    "                \n",
    "                for i in range(42, 47):\n",
    "                    cv2.line(result, tuple(landmarks[i]), tuple(landmarks[i+1]), color, 1)\n",
    "                cv2.line(result, tuple(landmarks[47]), tuple(landmarks[42]), color, 1)\n",
    "                \n",
    "                # Outer lips\n",
    "                for i in range(48, 59):\n",
    "                    cv2.line(result, tuple(landmarks[i]), tuple(landmarks[i+1]), color, 1)\n",
    "                cv2.line(result, tuple(landmarks[59]), tuple(landmarks[48]), color, 1)\n",
    "                \n",
    "                # Inner lips\n",
    "                if num_landmarks >= 68:\n",
    "                    for i in range(60, 67):\n",
    "                        cv2.line(result, tuple(landmarks[i]), tuple(landmarks[i+1]), color, 1)\n",
    "                    cv2.line(result, tuple(landmarks[67]), tuple(landmarks[60]), color, 1)\n",
    "                \n",
    "                # Draw forehead line if forehead points exist\n",
    "                if num_landmarks > 68:\n",
    "                    # Connect forehead points\n",
    "                    for i in range(68, num_landmarks - 1):\n",
    "                        cv2.line(result, tuple(landmarks[i]), tuple(landmarks[i+1]), (0, 0, 255), 1)\n",
    "        \n",
    "        return result\n",
    "\n",
    "print(\"‚úì FaceLandmarkDetector class defined (using face-alignment)!\")\n",
    "print(\"  üÜï Les landmarks incluent maintenant le FRONT par d√©faut (73 points)!\")\n",
    "print(\"  üìç Points 0-67: Landmarks faciaux standard\")\n",
    "print(\"  üìç Points 68-72: Points du front (en ROUGE dans la visualisation)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5140911f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-20T14:01:56.054313Z",
     "start_time": "2025-11-20T14:01:48.782399Z"
    }
   },
   "outputs": [],
   "source": [
    "# Initialize landmark detector with face-alignment\n",
    "landmark_detector = FaceLandmarkDetector(fa)\n",
    "\n",
    "print(\"‚úì Landmark detector is ready!\")\n",
    "print(\"Using face-alignment for state-of-the-art landmark detection.\")\n",
    "print(\"\\nüéØ IMPORTANT: Les landmarks incluent maintenant le FRONT automatiquement!\")\n",
    "print(\"   ‚Ä¢ 68 points standard + 5 points de front = 73 points au total\")\n",
    "print(\"   ‚Ä¢ Points 68-72 (en ROUGE) = zone du front\")\n",
    "\n",
    "# Example usage (uncomment to test with your image):\n",
    "if 'image' in globals() and image is not None and 'faces' in globals() and len(faces) > 0:\n",
    "    # Get landmarks using face-alignment (avec front inclus par d√©faut)\n",
    "    landmarks = landmark_detector.get_landmarks(image, include_forehead=True)\n",
    "    \n",
    "    if landmarks is not None:\n",
    "        print(f\"\\n‚úì D√©tection r√©ussie!\")\n",
    "        print(f\"   ‚Ä¢ Nombre de landmarks: {len(landmarks)} points\")\n",
    "        print(f\"   ‚Ä¢ Points 0-67: Visage standard\")\n",
    "        print(f\"   ‚Ä¢ Points 68-72: Front (nouveaux!)\")\n",
    "        \n",
    "        # Draw landmarks\n",
    "        result = landmark_detector.draw_landmarks(image, landmarks)\n",
    "        display_image(result, f\"Facial Landmarks avec Front ({len(landmarks)} points)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3252cb24",
   "metadata": {},
   "source": [
    "## 4. Face Swapping <a name=\"swapping\"></a>\n",
    "\n",
    "Now comes the exciting part - face swapping! We'll implement the core algorithm:\n",
    "1. Detect faces and landmarks in both source and target images\n",
    "2. **‚ú® NEW: Extend landmarks to include forehead area**\n",
    "3. **üëÑ NEW: Detect open mouth and preserve inner mouth area**\n",
    "4. Align the faces using landmarks\n",
    "5. Create a mask for seamless blending\n",
    "6. Swap the faces using triangulation and warping\n",
    "7. Blend the result using Poisson blending or similar techniques\n",
    "\n",
    "### üÜï Am√©liorations Principales\n",
    "\n",
    "#### 1. Inclusion du Front\n",
    "Le syst√®me a √©t√© am√©lior√© pour **inclure le front** dans la zone de remplacement :\n",
    "- **Extension automatique** : 5 points suppl√©mentaires sont ajout√©s au-dessus des sourcils\n",
    "- **Couverture compl√®te** : Le front est maintenant enti√®rement remplac√© lors du swap\n",
    "- **R√©sultat naturel** : Le blending inclut toute la zone frontale pour un r√©sultat plus coh√©rent\n",
    "\n",
    "#### 2. Pr√©servation Intelligente de la Bouche üëÑ\n",
    "**Nouvelle fonctionnalit√©** : D√©tection automatique de bouche ouverte et pr√©servation de l'int√©rieur\n",
    "- **D√©tection automatique** : Analyse du ratio d'ouverture de la bouche (hauteur/largeur)\n",
    "- **Seuil intelligent** : Bouche consid√©r√©e ouverte si ratio > 5%\n",
    "- **Pr√©servation cibl√©e** : Seul l'int√©rieur de la bouche (points 60-67) est exclu du remplacement\n",
    "- **R√©sultat r√©aliste** : Conserve les dents, la langue et l'int√©rieur de la bouche d'origine\n",
    "- **Activation par d√©faut** : Param√®tre `preserve_mouth=True` dans `swap_faces()`\n",
    "\n",
    "**Pourquoi c'est important ?**\n",
    "- √âvite les artefacts visuels d√©sagr√©ables (dents mal align√©es, langue d√©form√©e)\n",
    "- Conserve l'expression naturelle de la personne\n",
    "- Am√©liore consid√©rablement le r√©alisme pour les photos avec bouche ouverte"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e6bb65e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-20T14:01:56.137002Z",
     "start_time": "2025-11-20T14:01:56.119314Z"
    }
   },
   "outputs": [],
   "source": [
    "class FaceSwapper:\n",
    "    \"\"\"\n",
    "    A class for swapping faces between images.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        Initialize the face swapper.\n",
    "        \"\"\"\n",
    "        pass\n",
    "    \n",
    "    def is_mouth_open(self, landmarks, threshold=0.05):\n",
    "        \"\"\"\n",
    "        Detect if the mouth is open based on landmarks.\n",
    "        \n",
    "        Args:\n",
    "            landmarks: Facial landmark points (68 points)\n",
    "            threshold: Ratio threshold for mouth opening (default 0.05 = 5%)\n",
    "            \n",
    "        Returns:\n",
    "            Boolean indicating if mouth is open\n",
    "        \"\"\"\n",
    "        if len(landmarks) < 68:\n",
    "            return False\n",
    "        \n",
    "        # Inner mouth landmarks (standard 68-point model)\n",
    "        # Upper inner lip: points 61, 62, 63\n",
    "        # Lower inner lip: points 65, 66, 67\n",
    "        upper_lip_center = landmarks[62]  # Center top of inner lip\n",
    "        lower_lip_center = landmarks[66]  # Center bottom of inner lip\n",
    "        \n",
    "        # Mouth corners (for reference width)\n",
    "        left_corner = landmarks[48]\n",
    "        right_corner = landmarks[54]\n",
    "        \n",
    "        # Calculate mouth opening (vertical distance)\n",
    "        mouth_height = abs(lower_lip_center[1] - upper_lip_center[1])\n",
    "        \n",
    "        # Calculate mouth width (horizontal distance)\n",
    "        mouth_width = abs(right_corner[0] - left_corner[0])\n",
    "        \n",
    "        # Calculate ratio of height to width\n",
    "        if mouth_width > 0:\n",
    "            mouth_ratio = mouth_height / mouth_width\n",
    "        else:\n",
    "            mouth_ratio = 0\n",
    "        \n",
    "        # Mouth is considered open if ratio exceeds threshold\n",
    "        is_open = mouth_ratio > threshold\n",
    "        \n",
    "        return is_open, mouth_ratio\n",
    "    \n",
    "    def extend_landmarks_with_forehead(self, landmarks, image_shape):\n",
    "        \"\"\"\n",
    "        Extend facial landmarks to include forehead points.\n",
    "        \n",
    "        Args:\n",
    "            landmarks: Original facial landmark points (68 points)\n",
    "            image_shape: Shape of the image (height, width)\n",
    "            \n",
    "        Returns:\n",
    "            Extended landmarks including forehead points\n",
    "        \"\"\"\n",
    "        landmarks = np.array(landmarks, dtype=np.float32)\n",
    "        \n",
    "        # Get face outline points (indices 0-16 for standard 68-point model)\n",
    "        if len(landmarks) >= 17:\n",
    "            # Calculate forehead extension based on face outline\n",
    "            left_face = landmarks[0]  # Left side of face\n",
    "            right_face = landmarks[16]  # Right side of face\n",
    "            \n",
    "            # Get eyebrow points for reference (if available)\n",
    "            if len(landmarks) >= 27:\n",
    "                left_eyebrow_top = landmarks[19]  # Left eyebrow center\n",
    "                right_eyebrow_top = landmarks[24]  # Right eyebrow center\n",
    "                nose_bridge = landmarks[27]  # Top of nose bridge\n",
    "                \n",
    "                # Calculate forehead height (extend upward from eyebrows)\n",
    "                eyebrow_avg_y = (left_eyebrow_top[1] + right_eyebrow_top[1]) / 2\n",
    "                forehead_height = abs(nose_bridge[1] - eyebrow_avg_y) * 1.5\n",
    "                \n",
    "                # Create forehead points\n",
    "                forehead_points = []\n",
    "                \n",
    "                # Left forehead point\n",
    "                forehead_points.append([\n",
    "                    left_face[0] + (landmarks[1][0] - left_face[0]) * 0.3,\n",
    "                    eyebrow_avg_y - forehead_height\n",
    "                ])\n",
    "                \n",
    "                # Left-center forehead point\n",
    "                forehead_points.append([\n",
    "                    left_eyebrow_top[0],\n",
    "                    eyebrow_avg_y - forehead_height * 1.1\n",
    "                ])\n",
    "                \n",
    "                # Center forehead point\n",
    "                forehead_points.append([\n",
    "                    (left_eyebrow_top[0] + right_eyebrow_top[0]) / 2,\n",
    "                    eyebrow_avg_y - forehead_height * 1.2\n",
    "                ])\n",
    "                \n",
    "                # Right-center forehead point\n",
    "                forehead_points.append([\n",
    "                    right_eyebrow_top[0],\n",
    "                    eyebrow_avg_y - forehead_height * 1.1\n",
    "                ])\n",
    "                \n",
    "                # Right forehead point\n",
    "                forehead_points.append([\n",
    "                    right_face[0] - (right_face[0] - landmarks[15][0]) * 0.3,\n",
    "                    eyebrow_avg_y - forehead_height\n",
    "                ])\n",
    "                \n",
    "                # Add forehead points to landmarks\n",
    "                extended_landmarks = np.vstack([landmarks, forehead_points])\n",
    "                \n",
    "                return extended_landmarks.astype(np.int32)\n",
    "        \n",
    "        # Fallback: return original landmarks if extension fails\n",
    "        return landmarks.astype(np.int32)\n",
    "    \n",
    "    def get_convex_hull(self, landmarks):\n",
    "        \"\"\"\n",
    "        Get the convex hull of facial landmarks.\n",
    "        \n",
    "        Args:\n",
    "            landmarks: Array of facial landmark points\n",
    "            \n",
    "        Returns:\n",
    "            Convex hull points\n",
    "        \"\"\"\n",
    "        points = np.array(landmarks, dtype=np.int32)\n",
    "        hull = cv2.convexHull(points)\n",
    "        return hull\n",
    "    \n",
    "    def get_inner_mouth_mask(self, image_shape, landmarks):\n",
    "        \"\"\"\n",
    "        Create a mask for the inner mouth area.\n",
    "        \n",
    "        Args:\n",
    "            image_shape: Shape of the image (height, width)\n",
    "            landmarks: Facial landmark points (68 points)\n",
    "            \n",
    "        Returns:\n",
    "            Binary mask of inner mouth area\n",
    "        \"\"\"\n",
    "        mask = np.zeros(image_shape[:2], dtype=np.uint8)\n",
    "        \n",
    "        if len(landmarks) >= 68:\n",
    "            # Inner mouth contour (points 60-67 in 68-point model)\n",
    "            inner_mouth_points = landmarks[60:68].astype(np.int32)\n",
    "            \n",
    "            # Fill the inner mouth area\n",
    "            cv2.fillConvexPoly(mask, inner_mouth_points, 255)\n",
    "            \n",
    "            # Expand the mask slightly to ensure good coverage\n",
    "            kernel = np.ones((5, 5), np.uint8)\n",
    "            mask = cv2.dilate(mask, kernel, iterations=1)\n",
    "        \n",
    "        return mask\n",
    "    \n",
    "    def get_face_mask(self, image_shape, landmarks, include_forehead=True, exclude_inner_mouth=False):\n",
    "        \"\"\"\n",
    "        Create a mask for the face region, optionally including forehead and excluding inner mouth.\n",
    "        \n",
    "        Args:\n",
    "            image_shape: Shape of the image (height, width)\n",
    "            landmarks: Facial landmark points\n",
    "            include_forehead: Whether to extend mask to include forehead\n",
    "            exclude_inner_mouth: Whether to exclude the inner mouth area (for open mouths)\n",
    "            \n",
    "        Returns:\n",
    "            Binary mask\n",
    "        \"\"\"\n",
    "        # Extend landmarks to include forehead\n",
    "        if include_forehead:\n",
    "            extended_landmarks = self.extend_landmarks_with_forehead(landmarks, image_shape)\n",
    "        else:\n",
    "            extended_landmarks = landmarks\n",
    "        \n",
    "        mask = np.zeros(image_shape[:2], dtype=np.uint8)\n",
    "        hull = self.get_convex_hull(extended_landmarks)\n",
    "        cv2.fillConvexPoly(mask, hull, 255)\n",
    "        \n",
    "        # Exclude inner mouth if requested (for open mouths)\n",
    "        if exclude_inner_mouth and len(landmarks) >= 68:\n",
    "            inner_mouth_mask = self.get_inner_mouth_mask(image_shape, landmarks)\n",
    "            # Subtract inner mouth from face mask\n",
    "            mask = cv2.subtract(mask, inner_mouth_mask)\n",
    "        \n",
    "        # Optional: Apply slight blur to mask edges for smoother blending\n",
    "        mask = cv2.GaussianBlur(mask, (3, 3), 0)\n",
    "        \n",
    "        return mask\n",
    "    \n",
    "    def delaunay_triangulation(self, landmarks, image_shape):\n",
    "        \"\"\"\n",
    "        Perform Delaunay triangulation on facial landmarks.\n",
    "        \n",
    "        Args:\n",
    "            landmarks: Facial landmark points\n",
    "            image_shape: Shape of the image\n",
    "            \n",
    "        Returns:\n",
    "            List of triangles (indices)\n",
    "        \"\"\"\n",
    "        # Extend landmarks to include forehead\n",
    "        extended_landmarks = self.extend_landmarks_with_forehead(landmarks, image_shape)\n",
    "        \n",
    "        # Create Delaunay triangulation\n",
    "        tri = Delaunay(extended_landmarks)\n",
    "        return tri.simplices, extended_landmarks\n",
    "    \n",
    "    def warp_triangle(self, img1, img2, t1, t2):\n",
    "        \"\"\"\n",
    "        Warp a triangle from img1 to img2.\n",
    "        \n",
    "        Args:\n",
    "            img1: Source image\n",
    "            img2: Destination image\n",
    "            t1: Triangle vertices in img1\n",
    "            t2: Triangle vertices in img2\n",
    "        \"\"\"\n",
    "        # Find bounding rectangles\n",
    "        r1 = cv2.boundingRect(np.float32([t1]))\n",
    "        r2 = cv2.boundingRect(np.float32([t2]))\n",
    "        \n",
    "        # Offset points by left top corner of the respective rectangles\n",
    "        t1_rect = []\n",
    "        t2_rect = []\n",
    "        t2_rect_int = []\n",
    "        \n",
    "        for i in range(3):\n",
    "            t1_rect.append(((t1[i][0] - r1[0]), (t1[i][1] - r1[1])))\n",
    "            t2_rect.append(((t2[i][0] - r2[0]), (t2[i][1] - r2[1])))\n",
    "            t2_rect_int.append(((t2[i][0] - r2[0]), (t2[i][1] - r2[1])))\n",
    "        \n",
    "        # Get mask by filling triangle\n",
    "        mask = np.zeros((r2[3], r2[2], 3), dtype=np.float32)\n",
    "        cv2.fillConvexPoly(mask, np.int32(t2_rect_int), (1.0, 1.0, 1.0), 16, 0)\n",
    "        \n",
    "        # Apply warp to small rectangular patches\n",
    "        img1_rect = img1[r1[1]:r1[1] + r1[3], r1[0]:r1[0] + r1[2]]\n",
    "        \n",
    "        size = (r2[2], r2[3])\n",
    "        \n",
    "        # Get affine transform\n",
    "        warp_mat = cv2.getAffineTransform(np.float32(t1_rect), np.float32(t2_rect))\n",
    "        img2_rect = cv2.warpAffine(img1_rect, warp_mat, size, None, \n",
    "                                    flags=cv2.INTER_LINEAR, \n",
    "                                    borderMode=cv2.BORDER_REFLECT_101)\n",
    "        \n",
    "        # Apply mask and copy to destination\n",
    "        img2_rect = img2_rect * mask\n",
    "        \n",
    "        # Copy triangular region\n",
    "        img2[r2[1]:r2[1]+r2[3], r2[0]:r2[0]+r2[2]] = \\\n",
    "            img2[r2[1]:r2[1]+r2[3], r2[0]:r2[0]+r2[2]] * ((1.0, 1.0, 1.0) - mask)\n",
    "        \n",
    "        img2[r2[1]:r2[1]+r2[3], r2[0]:r2[0]+r2[2]] = \\\n",
    "            img2[r2[1]:r2[1]+r2[3], r2[0]:r2[0]+r2[2]] + img2_rect\n",
    "    \n",
    "    def swap_faces(self, src_image, src_landmarks, dst_image, dst_landmarks, preserve_mouth=True):\n",
    "        \"\"\"\n",
    "        Swap face from source image to destination image.\n",
    "        \n",
    "        Args:\n",
    "            src_image: Source image containing the face to swap\n",
    "            src_landmarks: Landmarks of the source face\n",
    "            dst_image: Destination image where face will be placed\n",
    "            dst_landmarks: Landmarks of the destination face\n",
    "            preserve_mouth: If True, preserve inner mouth when mouth is open\n",
    "            \n",
    "        Returns:\n",
    "            Image with swapped face\n",
    "        \"\"\"\n",
    "        # Convert images to float\n",
    "        src_img = np.float32(src_image)\n",
    "        dst_img = np.float32(dst_image)\n",
    "        result = dst_img.copy()\n",
    "        \n",
    "        # Check if mouth is open in destination\n",
    "        mouth_is_open = False\n",
    "        mouth_ratio = 0.0\n",
    "        \n",
    "        if preserve_mouth and len(dst_landmarks) >= 68:\n",
    "            mouth_is_open, mouth_ratio = self.is_mouth_open(dst_landmarks)\n",
    "            if mouth_is_open:\n",
    "                print(f\"  üëÑ Bouche ouverte d√©tect√©e (ratio: {mouth_ratio:.3f})\")\n",
    "                print(f\"  ‚úì L'int√©rieur de la bouche sera pr√©serv√©\")\n",
    "        \n",
    "        # Perform Delaunay triangulation on destination landmarks (with forehead)\n",
    "        triangles, dst_extended = self.delaunay_triangulation(dst_landmarks, dst_image.shape)\n",
    "        src_extended = self.extend_landmarks_with_forehead(src_landmarks, src_image.shape)\n",
    "        \n",
    "        # Warp each triangle\n",
    "        for triangle_indices in triangles:\n",
    "            # Get triangle vertices\n",
    "            t1 = [src_extended[i] for i in triangle_indices]\n",
    "            t2 = [dst_extended[i] for i in triangle_indices]\n",
    "            \n",
    "            # Warp triangle\n",
    "            self.warp_triangle(src_img, result, t1, t2)\n",
    "        \n",
    "        # Create mask for seamless cloning (including forehead, excluding inner mouth if open)\n",
    "        mask = self.get_face_mask(\n",
    "            dst_image.shape, \n",
    "            dst_landmarks, \n",
    "            include_forehead=True,\n",
    "            exclude_inner_mouth=mouth_is_open\n",
    "        )\n",
    "        \n",
    "        # Get center of the face for seamless cloning\n",
    "        r = cv2.boundingRect(self.get_convex_hull(dst_extended))\n",
    "        center = (r[0] + int(r[2] / 2), r[1] + int(r[3] / 2))\n",
    "        \n",
    "        # Seamless cloning\n",
    "        result = np.uint8(result)\n",
    "        output = cv2.seamlessClone(result, dst_image, mask, center, cv2.NORMAL_CLONE)\n",
    "        \n",
    "        return output\n",
    "\n",
    "print(\"‚úì FaceSwapper class defined!\")\n",
    "print(\"‚úì Face swapping now includes forehead in the replacement area!\")\n",
    "print(\"‚úì üÜï Intelligent mouth preservation: inner mouth is preserved when mouth is open!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da6c8fc5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-20T14:01:56.187161Z",
     "start_time": "2025-11-20T14:01:56.184036Z"
    }
   },
   "outputs": [],
   "source": [
    "# Initialize face swapper\n",
    "face_swapper = FaceSwapper()\n",
    "\n",
    "print(\"‚úì Face swapper initialized!\")\n",
    "print(\"\\nüÜï AM√âLIORATIONS:\")\n",
    "print(\"   1Ô∏è‚É£  Le syst√®me inclut maintenant le FRONT dans le remplacement\")\n",
    "print(\"       - Extension automatique des landmarks faciaux\")\n",
    "print(\"       - 5 points suppl√©mentaires ajout√©s pour couvrir le front\")\n",
    "print(\"       - Meilleure couverture de la zone faciale compl√®te\")\n",
    "print(\"\\n   2Ô∏è‚É£  üëÑ Pr√©servation intelligente de la bouche ouverte\")\n",
    "print(\"       - D√©tection automatique de l'ouverture de la bouche\")\n",
    "print(\"       - L'int√©rieur de la bouche est pr√©serv√© si elle est ouverte\")\n",
    "print(\"       - Conserve les dents, la langue et l'expression naturelle\")\n",
    "print(\"\\nReady to swap faces!\")\n",
    "print(\"Usage: result = face_swapper.swap_faces(src_img, src_landmarks, dst_img, dst_landmarks)\")\n",
    "print(\"Options: preserve_mouth=True (d√©faut) pour activer la pr√©servation de la bouche\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3c25bb7",
   "metadata": {},
   "source": [
    "### üé¨ Test sur la premi√®re frame d'une vid√©o\n",
    "\n",
    "Testons la d√©tection et les landmarks sur la premi√®re frame d'une vid√©o pour v√©rifier que tout fonctionne correctement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebe44f67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Charger la premi√®re frame d'une vid√©o pour tester\n",
    "TEST_VIDEO = '../assets/3People.mp4'  # Modifier ce chemin selon votre vid√©o\n",
    "\n",
    "test_frame = None\n",
    "test_landmarks = None\n",
    "test_faces = None\n",
    "\n",
    "if os.path.exists(TEST_VIDEO):\n",
    "    # Ouvrir la vid√©o\n",
    "    cap = cv2.VideoCapture(TEST_VIDEO)\n",
    "    \n",
    "    if cap.isOpened():\n",
    "        # Lire la premi√®re frame\n",
    "        ret, test_frame = cap.read()\n",
    "        cap.release()\n",
    "        \n",
    "        if ret:\n",
    "            print(f\"‚úì Premi√®re frame charg√©e depuis: {TEST_VIDEO}\")\n",
    "            print(f\"  Dimensions: {test_frame.shape[1]}x{test_frame.shape[0]}\")\n",
    "            \n",
    "            # D√©tecter les visages\n",
    "            test_faces = detector.detect(test_frame)\n",
    "            print(f\"  Visages d√©tect√©s: {len(test_faces)}\")\n",
    "            \n",
    "            # D√©tecter les landmarks sur le premier visage\n",
    "            if len(test_faces) > 0:\n",
    "                test_landmarks = landmark_detector.get_landmarks(test_frame, test_faces[0])\n",
    "                if test_landmarks is not None:\n",
    "                    print(f\"  Landmarks d√©tect√©s: {len(test_landmarks)} points\")\n",
    "            \n",
    "            # Afficher la frame avec les visages d√©tect√©s\n",
    "            frame_with_faces = detector.draw_faces(test_frame, test_faces)\n",
    "            display_image(frame_with_faces, f\"Premi√®re frame - {len(test_faces)} visage(s) d√©tect√©(s)\")\n",
    "        else:\n",
    "            print(f\"‚ùå Impossible de lire la premi√®re frame\")\n",
    "    else:\n",
    "        print(f\"‚ùå Impossible d'ouvrir la vid√©o: {TEST_VIDEO}\")\n",
    "else:\n",
    "    print(f\"‚ö† Vid√©o non trouv√©e: {TEST_VIDEO}\")\n",
    "    print(\"  Modifiez TEST_VIDEO pour pointer vers votre vid√©o\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "546ba708",
   "metadata": {},
   "source": [
    "### üìä Visualisation de la zone de remplacement avec le front\n",
    "\n",
    "Cette cellule permet de visualiser la zone qui sera remplac√©e sur la premi√®re frame, incluant le front."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfa672f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualiser la zone de remplacement avec le front sur la premi√®re frame\n",
    "if test_frame is not None and test_landmarks is not None:\n",
    "    # Cr√©er une copie de la frame\n",
    "    vis_frame = test_frame.copy()\n",
    "    \n",
    "    # √âtendre les landmarks pour inclure le front\n",
    "    extended_landmarks = face_swapper.extend_landmarks_with_forehead(test_landmarks, test_frame.shape)\n",
    "    \n",
    "    print(f\"Landmarks originaux: {len(test_landmarks)} points\")\n",
    "    print(f\"Landmarks √©tendus (avec front): {len(extended_landmarks)} points\")\n",
    "    \n",
    "    # Cr√©er le masque avec le front\n",
    "    mask_with_forehead = face_swapper.get_face_mask(test_frame.shape, test_landmarks, include_forehead=True)\n",
    "    mask_without_forehead = face_swapper.get_face_mask(test_frame.shape, test_landmarks, include_forehead=False)\n",
    "    \n",
    "    # Dessiner les landmarks originaux\n",
    "    frame_original = test_frame.copy()\n",
    "    for (x, y) in test_landmarks:\n",
    "        cv2.circle(frame_original, (int(x), int(y)), 2, (0, 255, 0), -1)\n",
    "    \n",
    "    # Dessiner les landmarks √©tendus (avec front en rouge)\n",
    "    frame_extended = test_frame.copy()\n",
    "    for (x, y) in test_landmarks:\n",
    "        cv2.circle(frame_extended, (int(x), int(y)), 2, (0, 255, 0), -1)\n",
    "    # Points du front en rouge\n",
    "    for i in range(len(test_landmarks), len(extended_landmarks)):\n",
    "        x, y = extended_landmarks[i]\n",
    "        cv2.circle(frame_extended, (int(x), int(y)), 3, (0, 0, 255), -1)\n",
    "    \n",
    "    # Cr√©er des visualisations de masques\n",
    "    mask_vis_without = cv2.cvtColor(mask_without_forehead, cv2.COLOR_GRAY2BGR)\n",
    "    mask_vis_without = cv2.addWeighted(test_frame, 0.6, mask_vis_without, 0.4, 0)\n",
    "    \n",
    "    mask_vis_with = cv2.cvtColor(mask_with_forehead, cv2.COLOR_GRAY2BGR)\n",
    "    mask_vis_with = cv2.addWeighted(test_frame, 0.6, mask_vis_with, 0.4, 0)\n",
    "    \n",
    "    # Afficher les r√©sultats\n",
    "    display_images_grid(\n",
    "        [frame_original, frame_extended, mask_vis_without, mask_vis_with],\n",
    "        [\n",
    "            'Landmarks originaux (68 points)',\n",
    "            'Landmarks avec front (points rouges)',\n",
    "            'Zone sans front',\n",
    "            'Zone AVEC front ‚úì'\n",
    "        ],\n",
    "        rows=2, cols=2,\n",
    "        figsize=(16, 12)\n",
    "    )\n",
    "    \n",
    "    print(\"\\n‚úì Visualisation sur la premi√®re frame de la vid√©o:\")\n",
    "    print(\"  - Les points VERTS sont les landmarks faciaux standard\")\n",
    "    print(\"  - Les points ROUGES sont les nouveaux points ajout√©s pour le front\")\n",
    "    print(\"  - La zone blanche en bas √† droite montre la zone qui sera remplac√©e\")\n",
    "    print(\"  - Le front est maintenant inclus dans le remplacement!\")\n",
    "else:\n",
    "    print(\"‚ö† Veuillez d'abord ex√©cuter la cellule pr√©c√©dente pour charger une frame de test\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ba71e3f",
   "metadata": {},
   "source": [
    "### üëÑ Test de d√©tection de bouche ouverte\n",
    "\n",
    "Cette cellule permet de tester la d√©tection automatique de bouche ouverte et la pr√©servation de l'int√©rieur de la bouche sur la premi√®re frame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dac928c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test de d√©tection de bouche ouverte et visualisation des masques sur la premi√®re frame\n",
    "if test_frame is not None and test_landmarks is not None:\n",
    "    # V√©rifier si la bouche est ouverte\n",
    "    mouth_is_open, mouth_ratio = face_swapper.is_mouth_open(test_landmarks)\n",
    "    \n",
    "    print(\"=\"*60)\n",
    "    print(\"ANALYSE DE LA BOUCHE (PREMI√àRE FRAME)\")\n",
    "    print(\"=\"*60)\n",
    "    print(f\"Bouche ouverte: {'OUI ‚úì' if mouth_is_open else 'NON'}\")\n",
    "    print(f\"Ratio d'ouverture: {mouth_ratio:.3f} ({mouth_ratio*100:.1f}%)\")\n",
    "    print(f\"Seuil de d√©tection: 0.05 (5%)\")\n",
    "    \n",
    "    if mouth_is_open:\n",
    "        print(\"\\nüéØ ACTION: L'int√©rieur de la bouche sera PR√âSERV√â lors du remplacement\")\n",
    "    else:\n",
    "        print(\"\\n‚Üí La bouche sera remplac√©e normalement\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # Cr√©er les diff√©rents masques pour visualisation\n",
    "    mask_with_mouth = face_swapper.get_face_mask(\n",
    "        test_frame.shape, \n",
    "        test_landmarks, \n",
    "        include_forehead=True,\n",
    "        exclude_inner_mouth=False\n",
    "    )\n",
    "    \n",
    "    mask_without_mouth = face_swapper.get_face_mask(\n",
    "        test_frame.shape, \n",
    "        test_landmarks, \n",
    "        include_forehead=True,\n",
    "        exclude_inner_mouth=True\n",
    "    )\n",
    "    \n",
    "    inner_mouth_mask = face_swapper.get_inner_mouth_mask(test_frame.shape, test_landmarks)\n",
    "    \n",
    "    # Visualiser les landmarks de la bouche\n",
    "    mouth_vis = test_frame.copy()\n",
    "    \n",
    "    # Dessiner les points de la bouche externe (48-59)\n",
    "    if len(test_landmarks) >= 68:\n",
    "        for i in range(48, 60):\n",
    "            cv2.circle(mouth_vis, tuple(test_landmarks[i]), 3, (0, 255, 0), -1)\n",
    "            cv2.putText(mouth_vis, str(i), tuple(test_landmarks[i]), \n",
    "                       cv2.FONT_HERSHEY_SIMPLEX, 0.3, (255, 255, 255), 1)\n",
    "        \n",
    "        # Dessiner les points de la bouche interne (60-67) en rouge\n",
    "        for i in range(60, 68):\n",
    "            cv2.circle(mouth_vis, tuple(test_landmarks[i]), 3, (0, 0, 255), -1)\n",
    "            cv2.putText(mouth_vis, str(i), tuple(test_landmarks[i]), \n",
    "                       cv2.FONT_HERSHEY_SIMPLEX, 0.3, (255, 255, 255), 1)\n",
    "        \n",
    "        # Ligne de mesure de l'ouverture\n",
    "        upper_lip = test_landmarks[62]\n",
    "        lower_lip = test_landmarks[66]\n",
    "        cv2.line(mouth_vis, tuple(upper_lip), tuple(lower_lip), (255, 0, 255), 2)\n",
    "        \n",
    "        # Afficher la distance\n",
    "        mouth_height = abs(lower_lip[1] - upper_lip[1])\n",
    "        cv2.putText(mouth_vis, f\"{mouth_height}px\", \n",
    "                   (int(upper_lip[0]) + 10, int((upper_lip[1] + lower_lip[1]) / 2)),\n",
    "                   cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 0, 255), 2)\n",
    "    \n",
    "    # Cr√©er des visualisations de masques\n",
    "    mask_vis_with = cv2.cvtColor(mask_with_mouth, cv2.COLOR_GRAY2BGR)\n",
    "    mask_vis_with = cv2.addWeighted(test_frame, 0.6, mask_vis_with, 0.4, 0)\n",
    "    \n",
    "    mask_vis_without = cv2.cvtColor(mask_without_mouth, cv2.COLOR_GRAY2BGR)\n",
    "    mask_vis_without = cv2.addWeighted(test_frame, 0.6, mask_vis_without, 0.4, 0)\n",
    "    \n",
    "    inner_mouth_vis = cv2.cvtColor(inner_mouth_mask, cv2.COLOR_GRAY2BGR)\n",
    "    inner_mouth_vis = cv2.addWeighted(test_frame, 0.6, inner_mouth_vis, 0.4, 0)\n",
    "    \n",
    "    # Afficher les r√©sultats\n",
    "    display_images_grid(\n",
    "        [mouth_vis, inner_mouth_vis, mask_vis_with, mask_vis_without],\n",
    "        [\n",
    "            'Landmarks de la bouche\\n(vert=externe, rouge=interne)',\n",
    "            'Zone de la bouche interne\\n(zone √† pr√©server)',\n",
    "            'Masque AVEC bouche\\n(remplacement complet)',\n",
    "            'Masque SANS bouche interne ‚úì\\n(pr√©serve l\\'int√©rieur)'\n",
    "        ],\n",
    "        rows=2, cols=2,\n",
    "        figsize=(16, 12)\n",
    "    )\n",
    "    \n",
    "    print(\"\\nüí° Explication:\")\n",
    "    print(\"  - Points VERTS (48-59): Contour externe de la bouche\")\n",
    "    print(\"  - Points ROUGES (60-67): Contour interne de la bouche\")\n",
    "    print(\"  - Ligne MAGENTA: Mesure de l'ouverture verticale\")\n",
    "    print(\"  - Zone BLANCHE en bas √† droite: Zone finale de remplacement\")\n",
    "    \n",
    "else:\n",
    "    print(\"‚ö† Veuillez d'abord ex√©cuter la cellule de chargement de la premi√®re frame\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad2f6460",
   "metadata": {},
   "source": [
    "### Visualisation de la zone de remplacement avec le front\n",
    "\n",
    "Cette cellule permet de visualiser la zone qui sera remplac√©e, incluant maintenant le front."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "957db62a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-20T14:01:56.613168Z",
     "start_time": "2025-11-20T14:01:56.243287Z"
    }
   },
   "outputs": [],
   "source": [
    "# Visualiser la zone de remplacement avec le front\n",
    "if 'image' in locals() and 'landmarks' in locals() and landmarks is not None:\n",
    "    # Cr√©er une copie de l'image\n",
    "    vis_image = image.copy()\n",
    "    \n",
    "    # √âtendre les landmarks pour inclure le front\n",
    "    extended_landmarks = face_swapper.extend_landmarks_with_forehead(landmarks, image.shape)\n",
    "    \n",
    "    print(f\"Landmarks originaux: {len(landmarks)} points\")\n",
    "    print(f\"Landmarks √©tendus (avec front): {len(extended_landmarks)} points\")\n",
    "    \n",
    "    # Cr√©er le masque avec le front\n",
    "    mask_with_forehead = face_swapper.get_face_mask(image.shape, landmarks, include_forehead=True)\n",
    "    mask_without_forehead = face_swapper.get_face_mask(image.shape, landmarks, include_forehead=False)\n",
    "    \n",
    "    # Dessiner les landmarks originaux\n",
    "    image_original = image.copy()\n",
    "    for (x, y) in landmarks:\n",
    "        cv2.circle(image_original, (int(x), int(y)), 2, (0, 255, 0), -1)\n",
    "    \n",
    "    # Dessiner les landmarks √©tendus (avec front en rouge)\n",
    "    image_extended = image.copy()\n",
    "    for (x, y) in landmarks:\n",
    "        cv2.circle(image_extended, (int(x), int(y)), 2, (0, 255, 0), -1)\n",
    "    # Points du front en rouge\n",
    "    for i in range(len(landmarks), len(extended_landmarks)):\n",
    "        x, y = extended_landmarks[i]\n",
    "        cv2.circle(image_extended, (int(x), int(y)), 3, (0, 0, 255), -1)\n",
    "    \n",
    "    # Cr√©er des visualisations de masques\n",
    "    mask_vis_without = cv2.cvtColor(mask_without_forehead, cv2.COLOR_GRAY2BGR)\n",
    "    mask_vis_without = cv2.addWeighted(image, 0.6, mask_vis_without, 0.4, 0)\n",
    "    \n",
    "    mask_vis_with = cv2.cvtColor(mask_with_forehead, cv2.COLOR_GRAY2BGR)\n",
    "    mask_vis_with = cv2.addWeighted(image, 0.6, mask_vis_with, 0.4, 0)\n",
    "    \n",
    "    # Afficher les r√©sultats\n",
    "    display_images_grid(\n",
    "        [image_original, image_extended, mask_vis_without, mask_vis_with],\n",
    "        [\n",
    "            'Landmarks originaux (68 points)',\n",
    "            'Landmarks avec front (points rouges)',\n",
    "            'Zone sans front',\n",
    "            'Zone AVEC front ‚úì'\n",
    "        ],\n",
    "        rows=2, cols=2,\n",
    "        figsize=(16, 12)\n",
    "    )\n",
    "    \n",
    "    print(\"\\n‚úì Comme vous pouvez le voir:\")\n",
    "    print(\"  - Les points ROUGES sont les nouveaux points ajout√©s pour le front\")\n",
    "    print(\"  - La zone blanche dans l'image du bas √† droite montre la zone qui sera remplac√©e\")\n",
    "    print(\"  - Le front est maintenant inclus dans le remplacement!\")\n",
    "else:\n",
    "    print(\"‚ö† Veuillez d'abord charger une image et d√©tecter les landmarks\")\n",
    "    print(\"   Ex√©cutez les cellules pr√©c√©dentes pour charger une image\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f545dee",
   "metadata": {},
   "source": [
    "### üëÑ Test de d√©tection de bouche ouverte\n",
    "\n",
    "Cette cellule permet de tester la d√©tection automatique de bouche ouverte et la pr√©servation de l'int√©rieur de la bouche."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baaf9a8c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-20T14:01:57.043416Z",
     "start_time": "2025-11-20T14:01:56.634901Z"
    }
   },
   "outputs": [],
   "source": [
    "# Test de d√©tection de bouche ouverte et visualisation des masques\n",
    "if 'image' in locals() and 'landmarks' in locals() and landmarks is not None:\n",
    "    # R√©initialiser le face swapper pour appliquer les nouvelles fonctionnalit√©s\n",
    "    face_swapper = FaceSwapper()\n",
    "    \n",
    "    # V√©rifier si la bouche est ouverte\n",
    "    mouth_is_open, mouth_ratio = face_swapper.is_mouth_open(landmarks)\n",
    "    \n",
    "    print(\"=\"*60)\n",
    "    print(\"ANALYSE DE LA BOUCHE\")\n",
    "    print(\"=\"*60)\n",
    "    print(f\"Bouche ouverte: {'OUI ‚úì' if mouth_is_open else 'NON'}\")\n",
    "    print(f\"Ratio d'ouverture: {mouth_ratio:.3f} ({mouth_ratio*100:.1f}%)\")\n",
    "    print(f\"Seuil de d√©tection: 0.05 (5%)\")\n",
    "    \n",
    "    if mouth_is_open:\n",
    "        print(\"\\nüéØ ACTION: L'int√©rieur de la bouche sera PR√âSERV√â lors du remplacement\")\n",
    "    else:\n",
    "        print(\"\\n‚Üí La bouche sera remplac√©e normalement\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # Cr√©er les diff√©rents masques pour visualisation\n",
    "    mask_with_mouth = face_swapper.get_face_mask(\n",
    "        image.shape, \n",
    "        landmarks, \n",
    "        include_forehead=True,\n",
    "        exclude_inner_mouth=False\n",
    "    )\n",
    "    \n",
    "    mask_without_mouth = face_swapper.get_face_mask(\n",
    "        image.shape, \n",
    "        landmarks, \n",
    "        include_forehead=True,\n",
    "        exclude_inner_mouth=True\n",
    "    )\n",
    "    \n",
    "    inner_mouth_mask = face_swapper.get_inner_mouth_mask(image.shape, landmarks)\n",
    "    \n",
    "    # Visualiser les landmarks de la bouche\n",
    "    mouth_vis = image.copy()\n",
    "    \n",
    "    # Dessiner les points de la bouche externe (48-59)\n",
    "    if len(landmarks) >= 68:\n",
    "        for i in range(48, 60):\n",
    "            cv2.circle(mouth_vis, tuple(landmarks[i]), 3, (0, 255, 0), -1)\n",
    "            cv2.putText(mouth_vis, str(i), tuple(landmarks[i]), \n",
    "                       cv2.FONT_HERSHEY_SIMPLEX, 0.3, (255, 255, 255), 1)\n",
    "        \n",
    "        # Dessiner les points de la bouche interne (60-67) en rouge\n",
    "        for i in range(60, 68):\n",
    "            cv2.circle(mouth_vis, tuple(landmarks[i]), 3, (0, 0, 255), -1)\n",
    "            cv2.putText(mouth_vis, str(i), tuple(landmarks[i]), \n",
    "                       cv2.FONT_HERSHEY_SIMPLEX, 0.3, (255, 255, 255), 1)\n",
    "        \n",
    "        # Ligne de mesure de l'ouverture\n",
    "        upper_lip = landmarks[62]\n",
    "        lower_lip = landmarks[66]\n",
    "        cv2.line(mouth_vis, tuple(upper_lip), tuple(lower_lip), (255, 0, 255), 2)\n",
    "        \n",
    "        # Afficher la distance\n",
    "        mouth_height = abs(lower_lip[1] - upper_lip[1])\n",
    "        cv2.putText(mouth_vis, f\"{mouth_height}px\", \n",
    "                   (int(upper_lip[0]) + 10, int((upper_lip[1] + lower_lip[1]) / 2)),\n",
    "                   cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 0, 255), 2)\n",
    "    \n",
    "    # Cr√©er des visualisations de masques\n",
    "    mask_vis_with = cv2.cvtColor(mask_with_mouth, cv2.COLOR_GRAY2BGR)\n",
    "    mask_vis_with = cv2.addWeighted(image, 0.6, mask_vis_with, 0.4, 0)\n",
    "    \n",
    "    mask_vis_without = cv2.cvtColor(mask_without_mouth, cv2.COLOR_GRAY2BGR)\n",
    "    mask_vis_without = cv2.addWeighted(image, 0.6, mask_vis_without, 0.4, 0)\n",
    "    \n",
    "    inner_mouth_vis = cv2.cvtColor(inner_mouth_mask, cv2.COLOR_GRAY2BGR)\n",
    "    inner_mouth_vis = cv2.addWeighted(image, 0.6, inner_mouth_vis, 0.4, 0)\n",
    "    \n",
    "    # Afficher les r√©sultats\n",
    "    display_images_grid(\n",
    "        [mouth_vis, inner_mouth_vis, mask_vis_with, mask_vis_without],\n",
    "        [\n",
    "            'Landmarks de la bouche\\n(vert=externe, rouge=interne)',\n",
    "            'Zone de la bouche interne\\n(zone √† pr√©server)',\n",
    "            'Masque AVEC bouche\\n(remplacement complet)',\n",
    "            'Masque SANS bouche interne ‚úì\\n(pr√©serve l\\'int√©rieur)'\n",
    "        ],\n",
    "        rows=2, cols=2,\n",
    "        figsize=(16, 12)\n",
    "    )\n",
    "    \n",
    "    print(\"\\nüí° Explication:\")\n",
    "    print(\"  - Points VERTS (48-59): Contour externe de la bouche\")\n",
    "    print(\"  - Points ROUGES (60-67): Contour interne de la bouche\")\n",
    "    print(\"  - Ligne MAGENTA: Mesure de l'ouverture verticale\")\n",
    "    print(\"  - Zone BLANCHE en bas √† droite: Zone finale de remplacement\")\n",
    "    \n",
    "else:\n",
    "    print(\"‚ö† Veuillez d'abord charger une image et d√©tecter les landmarks\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "315ff318",
   "metadata": {},
   "source": [
    "## 5. Complete Pipeline & Testing <a name=\"testing\"></a>\n",
    "\n",
    "Let's create a complete pipeline that combines all the components."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a945c56",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-20T14:01:57.074076Z",
     "start_time": "2025-11-20T14:01:57.066741Z"
    }
   },
   "outputs": [],
   "source": [
    "class VideoFaceAnonymizer:\n",
    "    \"\"\"\n",
    "    Complete pipeline for face detection and anonymization in videos.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, detector_method='haar'):\n",
    "        \"\"\"\n",
    "        Initialize the video face anonymizer.\n",
    "        \n",
    "        Args:\n",
    "            detector_method: Face detection method ('haar' or 'dnn')\n",
    "        \"\"\"\n",
    "        self.face_detector = FaceDetector(method=detector_method)\n",
    "        self.landmark_detector = FaceLandmarkDetector(fa)\n",
    "        self.face_swapper = FaceSwapper()\n",
    "        \n",
    "        # Statistics\n",
    "        self.stats = {\n",
    "            'total_frames': 0,\n",
    "            'frames_with_faces': 0,\n",
    "            'frames_without_faces': 0,\n",
    "            'total_faces_detected': 0,\n",
    "            'successful_swaps': 0,\n",
    "            'failed_swaps': 0\n",
    "        }\n",
    "        \n",
    "        print(\"‚úì Video Face Anonymizer initialized!\")\n",
    "    \n",
    "    def process_video(self, input_video_path, output_video_path, source_image=None, \n",
    "                     method='swap', show_preview=True, preview_interval=30):\n",
    "        \"\"\"\n",
    "        Process a video file and anonymize faces.\n",
    "        \n",
    "        Args:\n",
    "            input_video_path: Path to input video\n",
    "            output_video_path: Path to save output video\n",
    "            source_image: Source face image for swapping (if method='swap')\n",
    "            method: 'swap', 'blur', or 'pixelate'\n",
    "            show_preview: Show preview frames during processing\n",
    "            preview_interval: Show preview every N frames\n",
    "            \n",
    "        Returns:\n",
    "            Boolean indicating success\n",
    "        \"\"\"\n",
    "        print(\"=\"*60)\n",
    "        print(\"üé¨ STARTING VIDEO PROCESSING\")\n",
    "        print(\"=\"*60)\n",
    "        \n",
    "        # Open video file\n",
    "        cap = cv2.VideoCapture(input_video_path)\n",
    "        if not cap.isOpened():\n",
    "            print(f\"‚ùå Could not open video: {input_video_path}\")\n",
    "            return False\n",
    "        \n",
    "        # Get video properties\n",
    "        fps = int(cap.get(cv2.CAP_PROP_FPS))\n",
    "        width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "        height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "        total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "        \n",
    "        print(f\"\\nüìä Video Information:\")\n",
    "        print(f\"   Resolution: {width}x{height}\")\n",
    "        print(f\"   FPS: {fps}\")\n",
    "        print(f\"   Total frames: {total_frames}\")\n",
    "        print(f\"   Duration: {total_frames/fps:.2f} seconds\")\n",
    "        print(f\"   Method: {method}\")\n",
    "        \n",
    "        # Prepare source face if using swap method\n",
    "        source_landmarks = None\n",
    "        if method == 'swap' and source_image is not None:\n",
    "            print(f\"\\nüìÅ Loading source face...\")\n",
    "            source_faces = self.face_detector.detect(source_image)\n",
    "            if len(source_faces) > 0:\n",
    "                source_landmarks = self.landmark_detector.get_landmarks(source_image, source_faces[0])\n",
    "                if source_landmarks is not None:\n",
    "                    print(f\"‚úì Source face loaded with {len(source_landmarks)} landmarks\")\n",
    "                else:\n",
    "                    print(\"‚ö† Could not detect landmarks in source face\")\n",
    "                    print(\"   Falling back to blur method\")\n",
    "                    method = 'blur'\n",
    "            else:\n",
    "                print(\"‚ö† No face detected in source image\")\n",
    "                print(\"   Falling back to blur method\")\n",
    "                method = 'blur'\n",
    "        \n",
    "        # Setup video writer\n",
    "        fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "        out = cv2.VideoWriter(output_video_path, fourcc, fps, (width, height))\n",
    "        \n",
    "        if not out.isOpened():\n",
    "            print(f\"‚ùå Could not create output video: {output_video_path}\")\n",
    "            cap.release()\n",
    "            return False\n",
    "        \n",
    "        # Process frames\n",
    "        print(f\"\\n‚öôÔ∏è Processing frames...\")\n",
    "        frame_count = 0\n",
    "        start_time = time.time()\n",
    "        preview_frames = []\n",
    "        \n",
    "        with tqdm(total=total_frames, desc=\"Processing\", unit=\"frame\") as pbar:\n",
    "            while True:\n",
    "                ret, frame = cap.read()\n",
    "                if not ret:\n",
    "                    break\n",
    "                \n",
    "                self.stats['total_frames'] += 1\n",
    "                \n",
    "                # Detect faces in current frame\n",
    "                faces = self.face_detector.detect(frame)\n",
    "                \n",
    "                if len(faces) > 0:\n",
    "                    self.stats['frames_with_faces'] += 1\n",
    "                    self.stats['total_faces_detected'] += len(faces)\n",
    "                    \n",
    "                    # Process based on method\n",
    "                    if method == 'swap' and source_image is not None and source_landmarks is not None:\n",
    "                        # Try face swapping\n",
    "                        processed_frame = self._swap_faces_in_frame(\n",
    "                            frame, faces, source_image, source_landmarks\n",
    "                        )\n",
    "                    elif method == 'blur':\n",
    "                        processed_frame = self._blur_faces(frame, faces)\n",
    "                    elif method == 'pixelate':\n",
    "                        processed_frame = self._pixelate_faces(frame, faces)\n",
    "                    else:\n",
    "                        processed_frame = frame\n",
    "                else:\n",
    "                    self.stats['frames_without_faces'] += 1\n",
    "                    processed_frame = frame\n",
    "                \n",
    "                # Write frame\n",
    "                out.write(processed_frame)\n",
    "                \n",
    "                # Save preview frames\n",
    "                if show_preview and frame_count % preview_interval == 0:\n",
    "                    preview_frames.append((frame_count, frame.copy(), processed_frame.copy(), len(faces)))\n",
    "                \n",
    "                frame_count += 1\n",
    "                pbar.update(1)\n",
    "                \n",
    "                # Update progress bar with stats\n",
    "                if frame_count % 10 == 0:\n",
    "                    pbar.set_postfix({\n",
    "                        'faces': self.stats['total_faces_detected'],\n",
    "                        'swaps': self.stats['successful_swaps']\n",
    "                    })\n",
    "        \n",
    "        # Cleanup\n",
    "        cap.release()\n",
    "        out.release()\n",
    "        \n",
    "        elapsed_time = time.time() - start_time\n",
    "        \n",
    "        # Show statistics\n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(\"‚úÖ PROCESSING COMPLETE\")\n",
    "        print(\"=\"*60)\n",
    "        print(f\"\\nüìà Statistics:\")\n",
    "        print(f\"   Frames processed: {self.stats['total_frames']}\")\n",
    "        print(f\"   Frames with faces: {self.stats['frames_with_faces']}\")\n",
    "        print(f\"   Frames without faces: {self.stats['frames_without_faces']}\")\n",
    "        print(f\"   Total faces detected: {self.stats['total_faces_detected']}\")\n",
    "        if method == 'swap':\n",
    "            print(f\"   Successful swaps: {self.stats['successful_swaps']}\")\n",
    "            print(f\"   Failed swaps: {self.stats['failed_swaps']}\")\n",
    "        print(f\"\\n‚è±Ô∏è Processing time: {elapsed_time:.2f}s ({self.stats['total_frames']/elapsed_time:.2f} FPS)\")\n",
    "        print(f\"\\nüíæ Output saved: {output_video_path}\")\n",
    "        \n",
    "        # Show previews\n",
    "        if show_preview and len(preview_frames) > 0:\n",
    "            print(f\"\\nüñºÔ∏è Preview of {min(6, len(preview_frames))} frames:\")\n",
    "            self._show_preview_grid(preview_frames[:6])\n",
    "        \n",
    "        return True\n",
    "    \n",
    "    def _swap_faces_in_frame(self, frame, faces, source_image, source_landmarks):\n",
    "        \"\"\"Swap all detected faces in a frame\"\"\"\n",
    "        result = frame.copy()\n",
    "        \n",
    "        for face in faces:\n",
    "            try:\n",
    "                # Get landmarks for this face\n",
    "                target_landmarks = self.landmark_detector.get_landmarks(result, face)\n",
    "                \n",
    "                if target_landmarks is not None:\n",
    "                    # Perform face swap\n",
    "                    result = self.face_swapper.swap_faces(\n",
    "                        source_image,\n",
    "                        source_landmarks,\n",
    "                        result,\n",
    "                        target_landmarks\n",
    "                    )\n",
    "                    self.stats['successful_swaps'] += 1\n",
    "                else:\n",
    "                    self.stats['failed_swaps'] += 1\n",
    "            except Exception as e:\n",
    "                self.stats['failed_swaps'] += 1\n",
    "        \n",
    "        return result\n",
    "    \n",
    "    def _blur_faces(self, frame, faces):\n",
    "        \"\"\"Blur detected faces\"\"\"\n",
    "        result = frame.copy()\n",
    "        \n",
    "        for (x, y, w, h) in faces:\n",
    "            face_region = result[y:y+h, x:x+w]\n",
    "            face_region = cv2.GaussianBlur(face_region, (99, 99), 30)\n",
    "            result[y:y+h, x:x+w] = face_region\n",
    "        \n",
    "        return result\n",
    "    \n",
    "    def _pixelate_faces(self, frame, faces):\n",
    "        \"\"\"Pixelate detected faces\"\"\"\n",
    "        result = frame.copy()\n",
    "        \n",
    "        for (x, y, w, h) in faces:\n",
    "            face_region = result[y:y+h, x:x+w]\n",
    "            small = cv2.resize(face_region, (16, 16), interpolation=cv2.INTER_LINEAR)\n",
    "            face_region = cv2.resize(small, (w, h), interpolation=cv2.INTER_NEAREST)\n",
    "            result[y:y+h, x:x+w] = face_region\n",
    "        \n",
    "        return result\n",
    "    \n",
    "    def _show_preview_grid(self, preview_frames):\n",
    "        \"\"\"Display preview grid of processed frames\"\"\"\n",
    "        num_previews = len(preview_frames)\n",
    "        cols = 3\n",
    "        rows = (num_previews + cols - 1) // cols\n",
    "        \n",
    "        fig, axes = plt.subplots(rows, cols, figsize=(18, rows * 4))\n",
    "        axes = axes.flatten() if num_previews > 1 else [axes]\n",
    "        \n",
    "        for idx, (frame_num, original, processed, num_faces) in enumerate(preview_frames):\n",
    "            if idx < len(axes):\n",
    "                # Combine original and processed side by side\n",
    "                combined = np.hstack([\n",
    "                    cv2.cvtColor(original, cv2.COLOR_BGR2RGB),\n",
    "                    cv2.cvtColor(processed, cv2.COLOR_BGR2RGB)\n",
    "                ])\n",
    "                \n",
    "                axes[idx].imshow(combined)\n",
    "                axes[idx].set_title(f\"Frame {frame_num} | {num_faces} face(s)\\nLeft: Original | Right: Processed\", \n",
    "                                   fontsize=10)\n",
    "                axes[idx].axis('off')\n",
    "        \n",
    "        # Hide unused axes\n",
    "        for idx in range(num_previews, len(axes)):\n",
    "            axes[idx].axis('off')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "print(\"‚úì VideoFaceAnonymizer class defined!\")\n",
    "print(\"  ‚Üí Ready to process video files\")\n",
    "print(\"  ‚Üí Supports swap, blur, and pixelate methods\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42f4fab4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-20T14:01:57.261939Z",
     "start_time": "2025-11-20T14:01:57.125456Z"
    }
   },
   "outputs": [],
   "source": [
    "# Initialize the video anonymizer\n",
    "video_anonymizer = VideoFaceAnonymizer(detector_method='haar')\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"üé¨ Video Face Anonymization System Ready!\")\n",
    "print(\"=\"*60)\n",
    "print(\"\\nUsage Examples:\")\n",
    "print(\"\\n1. Blur faces in video:\")\n",
    "print(\"   video_anonymizer.process_video(\")\n",
    "print(\"       'input.mp4', 'output.mp4', method='blur')\")\n",
    "print(\"\\n2. Pixelate faces in video:\")\n",
    "print(\"   video_anonymizer.process_video(\")\n",
    "print(\"       'input.mp4', 'output.mp4', method='pixelate')\")\n",
    "print(\"\\n3. Face swap in video:\")\n",
    "print(\"   source_img = cv2.imread('source_face.jpg')\")\n",
    "print(\"   video_anonymizer.process_video(\")\n",
    "print(\"       'input.mp4', 'output.mp4', \")\n",
    "print(\"       source_image=source_img, method='swap')\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa59ccb4",
   "metadata": {},
   "source": [
    "### Test Face Swapping\n",
    "\n",
    "Now let's swap your detected face with another face! We'll need a source image containing the face we want to use."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9561fd5",
   "metadata": {},
   "source": [
    "## üé¨ Video Processing Examples\n",
    "\n",
    "### Example 1: Process a video with face swapping\n",
    "\n",
    "This example shows how to swap faces in a video using a source face image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "258d2354",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-20T14:01:57.273251227Z",
     "start_time": "2025-11-20T13:11:50.632967Z"
    }
   },
   "outputs": [],
   "source": [
    "# Configure video paths\n",
    "INPUT_VIDEO = '../assets/3People.mp4'  # Change to your video path\n",
    "OUTPUT_VIDEO = '../assets/output_anonymized.mp4'\n",
    "SOURCE_IMAGE_PATH = '../assets/Julien.png'  # Source face for swapping\n",
    "\n",
    "print(\"üìÅ Configuration:\")\n",
    "print(f\"   Input video: {INPUT_VIDEO}\")\n",
    "print(f\"   Output video: {OUTPUT_VIDEO}\")\n",
    "print(f\"   Source image: {SOURCE_IMAGE_PATH}\")\n",
    "\n",
    "# Check if input video exists\n",
    "if os.path.exists(INPUT_VIDEO):\n",
    "    print(f\"\\n‚úì Input video found!\")\n",
    "    \n",
    "    # Display video info\n",
    "    cap = cv2.VideoCapture(INPUT_VIDEO)\n",
    "    if cap.isOpened():\n",
    "        width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "        height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "        fps = int(cap.get(cv2.CAP_PROP_FPS))\n",
    "        frame_count = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "        duration = frame_count / fps if fps > 0 else 0\n",
    "        \n",
    "        print(f\"\\n   üìä Video information:\")\n",
    "        print(f\"      Resolution: {width}x{height}\")\n",
    "        print(f\"      FPS: {fps}\")\n",
    "        print(f\"      Frames: {frame_count}\")\n",
    "        print(f\"      Duration: {duration:.2f}s\")\n",
    "        \n",
    "        # Show first frame\n",
    "        ret, frame = cap.read()\n",
    "        if ret:\n",
    "            plt.figure(figsize=(10, 6))\n",
    "            plt.imshow(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB))\n",
    "            plt.title(\"First frame of input video\")\n",
    "            plt.axis('off')\n",
    "            plt.show()\n",
    "        \n",
    "        cap.release()\n",
    "    else:\n",
    "        print(f\"\\n‚ùå Could not open video!\")\n",
    "else:\n",
    "    print(f\"\\n‚ùå ERROR: Input video not found!\")\n",
    "    print(f\"   Please place your video at: {INPUT_VIDEO}\")\n",
    "\n",
    "# Load source image if using swap method\n",
    "source_image = None\n",
    "if os.path.exists(SOURCE_IMAGE_PATH):\n",
    "    source_image = cv2.imread(SOURCE_IMAGE_PATH)\n",
    "    if source_image is not None:\n",
    "        print(f\"\\n‚úì Source image loaded!\")\n",
    "        plt.figure(figsize=(6, 6))\n",
    "        plt.imshow(cv2.cvtColor(source_image, cv2.COLOR_BGR2RGB))\n",
    "        plt.title(\"Source Face for Swapping\")\n",
    "        plt.axis('off')\n",
    "        plt.show()\n",
    "    else:\n",
    "        print(f\"\\n‚ö† Could not load source image\")\n",
    "else:\n",
    "    print(f\"\\n‚ö† Source image not found: {SOURCE_IMAGE_PATH}\")\n",
    "    print(\"   Will use blur/pixelate methods instead\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a139d37b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-20T14:01:57.280693242Z",
     "start_time": "2025-11-20T13:11:50.770148Z"
    }
   },
   "outputs": [],
   "source": [
    "# Option: Download a test source image\n",
    "import urllib.request\n",
    "\n",
    "def download_test_face():\n",
    "    \"\"\"Download a sample face image for testing\"\"\"\n",
    "    test_url = \"https://raw.githubusercontent.com/opencv/opencv/master/samples/data/lena.jpg\"\n",
    "    output_path = \"test_source_face.jpg\"\n",
    "    \n",
    "    try:\n",
    "        print(\"Downloading test source image...\")\n",
    "        urllib.request.urlretrieve(test_url, output_path)\n",
    "        print(f\"‚úì Test image downloaded to: {output_path}\")\n",
    "        return output_path\n",
    "    except Exception as e:\n",
    "        print(f\"‚ö† Could not download: {e}\")\n",
    "        return None\n",
    "\n",
    "# Uncomment the lines below to download and use a test image:\n",
    "# source_path = download_test_face()\n",
    "# if source_path:\n",
    "#     source_image = cv2.imread(source_path)\n",
    "#     source_faces = detector.detect(source_image)\n",
    "#     display_image(detector.draw_faces(source_image, source_faces), \"Downloaded Test Image\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13370104",
   "metadata": {},
   "source": [
    "### Perform Face Swap\n",
    "\n",
    "Once you have both images loaded (target and source), run the cell below to perform the face swap!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05645632",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-20T14:01:57.282188781Z",
     "start_time": "2025-11-20T13:11:50.820636Z"
    }
   },
   "outputs": [],
   "source": [
    "# Process the video with face swapping\n",
    "if os.path.exists(INPUT_VIDEO):\n",
    "    print(\"Starting video processing with face swapping...\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # Choose method: 'swap', 'blur', or 'pixelate'\n",
    "    method = 'swap' if source_image is not None else 'blur'\n",
    "    \n",
    "    print(f\"Method: {method}\")\n",
    "    \n",
    "    # Process the video\n",
    "    success = video_anonymizer.process_video(\n",
    "        input_video_path=INPUT_VIDEO,\n",
    "        output_video_path=OUTPUT_VIDEO,\n",
    "        source_image=source_image,\n",
    "        method=method,\n",
    "        show_preview=True,\n",
    "        preview_interval=30\n",
    "    )\n",
    "    \n",
    "    if success:\n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(\"VIDEO PROCESSING SUCCESSFUL!\")\n",
    "        print(\"=\"*60)\n",
    "        print(f\"\\nProcessed video saved to: {OUTPUT_VIDEO}\")\n",
    "        print(\"\\nYou can now:\")\n",
    "        print(\"  1. Download the output video\")\n",
    "        print(\"  2. Play it with a video player\")\n",
    "        print(\"  3. Check the preview frames above\")\n",
    "    else:\n",
    "        print(\"\\nVideo processing failed. Check error messages above.\")\n",
    "else:\n",
    "    print(\"Please load an input video first!\")\n",
    "    print(f\"   Place your video at: {INPUT_VIDEO}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b289ea3",
   "metadata": {},
   "source": [
    "### Play the Output Video\n",
    "\n",
    "Display the processed video in the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b8e2007",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-20T14:01:57.283809355Z",
     "start_time": "2025-11-20T13:11:51.181260Z"
    }
   },
   "outputs": [],
   "source": [
    "# Play the output video in the notebook\n",
    "if os.path.exists(OUTPUT_VIDEO):\n",
    "    print(f\"üì∫ Playing output video: {OUTPUT_VIDEO}\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # Display video information\n",
    "    cap = cv2.VideoCapture(OUTPUT_VIDEO)\n",
    "    if cap.isOpened():\n",
    "        width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "        height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "        fps = int(cap.get(cv2.CAP_PROP_FPS))\n",
    "        frame_count = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "        duration = frame_count / fps if fps > 0 else 0\n",
    "        \n",
    "        print(f\"üìä Output video information:\")\n",
    "        print(f\"   Resolution: {width}x{height}\")\n",
    "        print(f\"   FPS: {fps}\")\n",
    "        print(f\"   Frames: {frame_count}\")\n",
    "        print(f\"   Duration: {duration:.2f}s\")\n",
    "        print(f\"   File size: {os.path.getsize(OUTPUT_VIDEO) / (1024*1024):.2f} MB\")\n",
    "        \n",
    "        cap.release()\n",
    "    \n",
    "    # Display the video using IPython\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"üé¨ VIDEO PLAYER\")\n",
    "    print(\"=\"*60)\n",
    "    display(Video(OUTPUT_VIDEO, width=800))\n",
    "    \n",
    "else:\n",
    "    print(f\"‚ùå Output video not found: {OUTPUT_VIDEO}\")\n",
    "    print(\"   Please run the video processing cell first!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71efc71c",
   "metadata": {},
   "source": [
    "## üéâ Video Face Anonymization Complete!\n",
    "\n",
    "### ‚úÖ What This Notebook Does\n",
    "\n",
    "This notebook provides a complete **video-based face anonymization system** with the following features:\n",
    "\n",
    "#### üé¨ Video Processing Capabilities:\n",
    "- **Frame-by-frame processing** with progress bars\n",
    "- **Multiple anonymization methods**:\n",
    "  - üîÑ **Face Swapping**: Replace faces with a source face\n",
    "  - üå´Ô∏è **Blur**: Gaussian blur on detected faces\n",
    "  - üì¶ **Pixelate**: Pixelation effect on faces\n",
    "- **Real-time preview**: Preview frames during processing\n",
    "- **Statistics tracking**: Face detection and swap success rates\n",
    "- **MP4 output**: Compatible video format with preserved FPS\n",
    "\n",
    "#### üÜï Advanced Features:\n",
    "- **Forehead inclusion**: 73-point facial landmarks (68 standard + 5 forehead)\n",
    "- **Intelligent mouth preservation**: Automatically preserves inner mouth when open\n",
    "- **Delaunay triangulation**: Professional face warping algorithm\n",
    "- **face-alignment library**: State-of-the-art facial landmark detection\n",
    "- **GPU acceleration support**: CUDA-enabled when available\n",
    "\n",
    "### üöÄ Usage Guide\n",
    "\n",
    "1. **Configure your video paths** (Cell: Configuration)\n",
    "2. **Load input video and source face** (Cell: Load Video)\n",
    "3. **Process the video** (Cell: Process Video)\n",
    "4. **Watch the result** (Cell: Play Output Video)\n",
    "\n",
    "### üìù Next Steps\n",
    "\n",
    "To enhance this project further:\n",
    "\n",
    "1. **Add more anonymization methods**: Emoji overlay, artistic filters, etc.\n",
    "2. **Batch processing**: Process multiple videos at once\n",
    "3. **Audio preservation**: Keep original audio track in output\n",
    "4. **Real-time processing**: Optimize for live video streams\n",
    "5. **Face tracking**: Track individual faces across frames for consistent swapping\n",
    "6. **Quality metrics**: Implement evaluation metrics for anonymization quality\n",
    "7. **Web interface**: Create a web app for easy video upload and processing\n",
    "\n",
    "### üõ†Ô∏è Technical Stack\n",
    "\n",
    "- ‚úÖ **Python 3.13 compatible**\n",
    "- ‚úÖ **OpenCV**: Video processing and face detection\n",
    "- ‚úÖ **face-alignment**: Advanced facial landmark detection\n",
    "- ‚úÖ **SciPy**: Delaunay triangulation\n",
    "- ‚úÖ **NumPy & Matplotlib**: Data processing and visualization\n",
    "- ‚úÖ **tqdm**: Progress bars for long operations\n",
    "\n",
    "### üì¶ Required Files\n",
    "\n",
    "- `lbfmodel.yaml` - Facial landmark model (auto-downloaded by face-alignment)\n",
    "- Input video (MP4 format recommended)\n",
    "- Source face image (JPG, PNG) for face swapping\n",
    "\n",
    "### üéØ Project Status\n",
    "\n",
    "This is a fully functional video face anonymization system ready for production use! üöÄ\n",
    "\n",
    "Good luck with your computer vision project! \udcaa"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7302b8f",
   "metadata": {},
   "source": [
    "### üé® Quick Processing with Different Methods\n",
    "\n",
    "Use these shortcuts to quickly process videos with different anonymization methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd76eea8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick method shortcuts - uncomment the one you want to use\n",
    "\n",
    "# Example 1: Face swapping with custom source and output paths\n",
    "# video_anonymizer.process_video(\n",
    "#     input_video_path='../assets/my_video.mp4',\n",
    "#     output_video_path='../assets/swapped_output.mp4',\n",
    "#     source_image=cv2.imread('../assets/my_source_face.jpg'),\n",
    "#     method='swap',\n",
    "#     show_preview=True,\n",
    "#     preview_interval=30\n",
    "# )\n",
    "\n",
    "# Example 2: Blur all faces (no source image needed)\n",
    "# video_anonymizer.process_video(\n",
    "#     input_video_path='../assets/my_video.mp4',\n",
    "#     output_video_path='../assets/blurred_output.mp4',\n",
    "#     method='blur',\n",
    "#     show_preview=True,\n",
    "#     preview_interval=30\n",
    "# )\n",
    "\n",
    "# Example 3: Pixelate all faces (no source image needed)\n",
    "# video_anonymizer.process_video(\n",
    "#     input_video_path='../assets/my_video.mp4',\n",
    "#     output_video_path='../assets/pixelated_output.mp4',\n",
    "#     method='pixelate',\n",
    "#     show_preview=True,\n",
    "#     preview_interval=30\n",
    "# )\n",
    "\n",
    "print(\"üí° TIP: Uncomment one of the examples above to process a video!\")\n",
    "print(\"üìù Remember to:\")\n",
    "print(\"   1. Change the input/output paths to your files\")\n",
    "print(\"   2. Set the correct method: 'swap', 'blur', or 'pixelate'\")\n",
    "print(\"   3. Load a source image if using 'swap' method\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
